<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta name="keywords" content="Speech synthesis,MS Sam.ogg,1003,1198,1214,1280,1294,1779,1791,1837,1857" />
<link rel="shortcut icon"  />
<link rel="search" type="application/opensearchdescription+xml"  />
<link rel="copyright"  />
		<title>Speech synthesis - Wikipedia, the free encyclopedia</title>
		<style type="text/css" media="screen,projection">/*<![CDATA[*/ @import "/skins-1.5/monobook/main.css?9"; /*]]>*/</style>
		<link rel="stylesheet" type="text/css" media="print"  />
		<!--[if lt IE 5.5000]><style type="text/css">@import "/skins-1.5/monobook/IE50Fixes.css";</style><![endif]-->
		<!--[if IE 5.5000]><style type="text/css">@import "/skins-1.5/monobook/IE55Fixes.css";</style><![endif]-->
		<!--[if IE 6]><style type="text/css">@import "/skins-1.5/monobook/IE60Fixes.css";</style><![endif]-->
		<!--[if IE 7]><style type="text/css">@import "/skins-1.5/monobook/IE70Fixes.css?1";</style><![endif]-->
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->
		
		<script type= "text/javascript">
			var skin = "monobook";
			var stylepath = "/skins-1.5";

			var wgArticlePath = "/wiki/$1";
			var wgScriptPath = "/w";
			var wgServer = "http://en.wikipedia.org";
                        
			var wgCanonicalNamespace = "";
			var wgNamespaceNumber = 0;
			var wgPageName = "Speech_synthesis";
			var wgTitle = "Speech synthesis";
			var wgArticleId = 42799;
			var wgIsArticle = true;
                        
			var wgUserName = null;
			var wgUserLanguage = "en";
			var wgContentLanguage = "en";
		</script>
		                
		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?1"><!-- wikibits js --></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js"><!-- site js --></script>
		<style type="text/css">/*<![CDATA[*/
@import "/w/index.php?title=MediaWiki:Common.css&usemsgcache=yes&action=raw&ctype=text/css&smaxage=2678400";
@import "/w/index.php?title=MediaWiki:Monobook.css&usemsgcache=yes&action=raw&ctype=text/css&smaxage=2678400";
@import "/w/index.php?title=-&action=raw&gen=css&maxage=2678400";
/*]]>*/</style>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js"></script>
	</head>
<body  class="mediawiki ns-0 ltr">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><div style="text-align:right; font-size:80%">Your <b><a  class="extiw" title="wikimedia:Fundraising">continued donations</a></b> keep Wikipedia running!&nbsp;&nbsp;&nbsp;&nbsp;</div>
</div>		<h1 class="firstHeading">Speech synthesis</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub">(Redirected from <a  title="Speech synthesizer">Speech synthesizer</a>)</div>
									<div id="jump-to-nav">Jump to: <a >navigation</a>, <a >search</a></div>			<!-- start content -->
			<div style="right:10px; display:none;" class="metadata topicon" id="featured-star">
<div style="position: relative; width: 14px; height: 14px; overflow: hidden">
<div style="position: absolute; top: 0px; left: 0px; font-size: 100px; overflow: hidden; line-height: 100px; z-index: 3"><a  title="Wikipedia:Featured articles"><span title="This is a featured article. Click here for more information.">&#160;&#160;&#160;</span></a></div>
<div style="position: absolute; top: 0px; left: 0px; z-index: 2"><a  class="image" title="This is a featured article. Click here for more information."><img src="http://upload.wikimedia.org/wikipedia/en/6/60/LinkFA-star.png" alt="This is a featured article. Click here for more information." width="14" height="14" longdesc="/wiki/Image:LinkFA-star.png" /></a></div>
</div>
</div>
<p><b>Speech synthesis</b> is the artificial production of human <a href="/wiki/Speech.html" title="Speech">speech</a>. A system used for this purpose is termed a <b>speech synthesizer</b>, and can be implemented in <a href="/wiki/Software.html" title="Software">software</a> or <a href="/wiki/Hardware.html" title="Hardware">hardware</a>. Speech synthesis systems are often called <b>text-to-speech (TTS)</b> systems in reference to their ability to convert text into speech. However, systems exist that instead render <a href="/wiki/Symbolic_linguistic_representation.html" title="Symbolic linguistic representation">symbolic linguistic representations</a> like <a href="/wiki/Phonetic_transcription.html" title="Phonetic transcription">phonetic transcriptions</a> into speech. Text-to-speech software provides many benefits. Children and adults who struggle with reading printed text due to visual impairments or reading disabilities greatly benefit from the use of text-to-speech software. Obstacles to independently accessing printed text are removed when a computer generated voice is attached to text, whether in a word processing program or on the Internet.</p>
<div class="medialist listenlist">
<ul>
<li><a  class="internal" title="MS Sam.ogg">Sample of Microsoft Sam</a> (<a  title="Image:MS Sam.ogg">file info</a>) — <span class="plainlinks"><a  class="external text" title="http://tools.wikimedia.de/~gmaxwell/jorbis/JOrbisPlayer.php?path=MS+Sam.ogg&amp;wiki=en">play in browser</a> <small>(<a href="/wiki/Development_stage#Beta.html" title="Development stage">beta</a>)</small></span>
<ul>
<li><a href="/wiki/Windows_XP.html" title="Windows XP">Windows XP</a>’s default voice saying, “The quick brown fox jumps over the lazy dog 1,234,567,890 times. soif”</li>
<li><i>Problems listening to the file? See <a  title="Wikipedia:Media help">media help</a>.</i></li>
</ul>
</li>
</ul>
</div>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a ><span class="tocnumber">1</span> <span class="toctext">Overview of speech synthesis technology</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">2</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">3</span> <span class="toctext">Synthesizer technologies</span></a>
<ul>
<li class="toclevel-2"><a ><span class="tocnumber">3.1</span> <span class="toctext">Concatenative synthesis</span></a>
<ul>
<li class="toclevel-3"><a ><span class="tocnumber">3.1.1</span> <span class="toctext">Unit selection synthesis</span></a></li>
<li class="toclevel-3"><a ><span class="tocnumber">3.1.2</span> <span class="toctext">Diphone synthesis</span></a></li>
<li class="toclevel-3"><a ><span class="tocnumber">3.1.3</span> <span class="toctext">Domain-specific synthesis</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a ><span class="tocnumber">3.2</span> <span class="toctext">Formant synthesis</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">3.3</span> <span class="toctext">Articulatory synthesis</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">3.4</span> <span class="toctext">Hybrid synthesis</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">3.5</span> <span class="toctext">HMM-based synthesis</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a ><span class="tocnumber">4</span> <span class="toctext">Front-end challenges</span></a>
<ul>
<li class="toclevel-2"><a ><span class="tocnumber">4.1</span> <span class="toctext">Text normalization challenges</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">4.2</span> <span class="toctext">Text-to-phoneme challenges</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a ><span class="tocnumber">5</span> <span class="toctext">Speech synthesis markup languages</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">6</span> <span class="toctext">Operating systems featuring speech synthesis</span></a>
<ul>
<li class="toclevel-2"><a ><span class="tocnumber">6.1</span> <span class="toctext">Apple</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">6.2</span> <span class="toctext">AmigaOS</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">6.3</span> <span class="toctext">Windows</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">6.4</span> <span class="toctext">GNU/Linux</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">6.5</span> <span class="toctext">Texas Instruments TI-99/4 and TI-99/4A</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">6.6</span> <span class="toctext">Third party systems</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a ><span class="tocnumber">7</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">9</span> <span class="toctext">External links</span></a>
<ul>
<li class="toclevel-2"><a ><span class="tocnumber">9.1</span> <span class="toctext">Misc</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">9.2</span> <span class="toctext">Freely available TTS systems</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">9.3</span> <span class="toctext">Commercially available TTS systems</span></a></li>
</ul>
</li>
</ul>
</td>
</tr>
</table>
<p><script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script></p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Overview of speech synthesis technology">edit</a>]</div>
<p><a name="Overview_of_speech_synthesis_technology" id="Overview_of_speech_synthesis_technology"></a></p>
<h2>Overview of speech synthesis technology</h2>
<p>A <b>text-to-speech system</b> (or engine) is composed of two parts: a <b>front-end</b> and a <b>back-end</b>. Broadly, the front-end takes <a href="/wiki/Input.html" title="Input">input</a> in the form of text and <a href="/wiki/Output.html" title="Output">outputs</a> a <a href="/wiki/Symbolic_linguistic_representation.html" title="Symbolic linguistic representation">symbolic linguistic representation</a>. The back-end takes the symbolic linguistic representation as input and outputs the synthesized speech <a href="/wiki/Waveform.html" title="Waveform">waveform</a>.</p>
<p>The <a href="/wiki/Front-end.html" title="Front-end">front-end</a> has two major tasks. First, it takes the raw text and converts things like numbers and abbreviations into their written-out word equivalents. This process is often called <i>text normalization</i>, <i>pre-processing</i>, or <i>tokenization</i>. Then it assigns <a href="/wiki/Phonetic_transcription.html" title="Phonetic transcription">phonetic transcriptions</a> to each word, and divides and marks the text into various <a href="/wiki/Prosody_%28linguistics%29.html" title="Prosody (linguistics)">prosodic units</a>, like <a href="/wiki/Phrase.html" title="Phrase">phrases</a>, <a href="/wiki/Clause.html" title="Clause">clauses</a>, and <a href="/wiki/Sentence_%28linguistics%29.html" title="Sentence (linguistics)">sentences</a>. The process of assigning phonetic transcriptions to words is called <i>text-to-phoneme (TTP)</i> or <i><a href="/wiki/Grapheme.html" title="Grapheme">grapheme</a>-to-phoneme (GTP)</i> conversion. The combination of phonetic transcriptions and prosody information make up the <i>symbolic linguistic representation</i> output of the front end.</p>
<p>The other part, the back-end, takes the symbolic linguistic representation and converts it into actual sound output. The back end is often referred to as the <b>synthesizer</b>. The different techniques synthesizers use are described below.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: History">edit</a>]</div>
<p><a name="History" id="History"></a></p>
<h2>History</h2>
<p>Long before modern electronic signal processing was invented, speech researchers tried to build machines to create human speech. Early examples of 'speaking heads' were made by <a href="/wiki/Pope_Silvester_II.html" title="Pope Silvester II">Gerbert of Aurillac</a> (d. <a href="/wiki/1003.html" title="1003">1003</a>), <a href="/wiki/Albertus_Magnus.html" title="Albertus Magnus">Albertus Magnus</a> (<a href="/wiki/1198.html" title="1198">1198</a>–<a href="/wiki/1280.html" title="1280">1280</a>), and <a href="/wiki/Roger_Bacon.html" title="Roger Bacon">Roger Bacon</a> (<a href="/wiki/1214.html" title="1214">1214</a>–<a href="/wiki/1294.html" title="1294">1294</a>).</p>
<p>In <a href="/wiki/1779.html" title="1779">1779</a>, the Danish scientist <a  class="new" title="Christian Gottlieb Kratzenstein">Christian Kratzenstein</a>, working at that time at the <a href="/wiki/Russian_Academy_of_Sciences.html" title="Russian Academy of Sciences">Russian Academy of Sciences</a>, built models of the human vocal tract that could produce the five long <a href="/wiki/Vowel.html" title="Vowel">vowel</a> sounds ([a], [e], [I], [o] and [u]). This was followed by the bellows-operated 'Acoustic-Mechanical Speech Machine' by <a href="/wiki/Wolfgang_von_Kempelen.html" title="Wolfgang von Kempelen">Wolfgang von Kempelen</a> of <a href="/wiki/Vienna.html" title="Vienna">Vienna</a>, <a href="/wiki/Austria.html" title="Austria">Austria</a>, described in his <a href="/wiki/1791.html" title="1791">1791</a> paper <i><a  class="new" title="Mechanismus der menschlichen Sprache nebst der Beschreibung seiner sprechenden Maschine">Mechanismus der menschlichen Sprache nebst der Beschreibung seiner sprechenden Maschine</a></i> ("mechanism of the human speech with description of its speaking machine," J. B. Degen, Wien). This machine added models of the tongue and lips, enabling it to produce <a href="/wiki/Consonant.html" title="Consonant">consonants</a> as well as vowels. In <a href="/wiki/1837.html" title="1837">1837</a> <a href="/wiki/Charles_Wheatstone.html" title="Charles Wheatstone">Charles Wheatstone</a> produced a 'speaking machine' based on von Kempelen's design, and in <a href="/wiki/1857.html" title="1857">1857</a> M. Faber built the 'Euphonia'. Wheatstone's design was resurrected in <a href="/wiki/1923.html" title="1923">1923</a> by Paget.</p>
<p>In the 1930s, <a href="/wiki/Bell_Labs.html" title="Bell Labs">Bell Labs</a> developed the <a href="/wiki/Vocoder.html" title="Vocoder">VOCODER</a>, a keyboard-operated electronic speech analyzer and synthesizer that was said to be clearly intelligible. <a  class="new" title="Homer Dudley">Homer Dudley</a> refined this device into the VODER, which he exhibited at the <a href="/wiki/1939_New_York_World%27s_Fair.html" title="1939 New York World's Fair">1939 New York World's Fair</a>.</p>
<p>Early electronic speech synthesizers sounded very robotic and were often barely intelligible. However, the quality of synthesized speech has steadily improved, and output from contemporary speech synthesis systems is sometimes indistinguishable from actual human speech.</p>
<p>Despite the success of purely electronic speech synthesis, research is still being conducted into mechanical speech synthesizers for use in humanoid <a href="/wiki/Robot.html" title="Robot">robots</a>. Even a perfect electronic synthesizer is limited by the quality of the <a href="/wiki/Transducer.html" title="Transducer">transducer</a> (usually a <a href="/wiki/Loudspeaker.html" title="Loudspeaker">loudspeaker</a>) that produces the sound, so, in a robot, a mechanical system may be able to produce a more natural sound than a small loudspeaker.</p>
<p>The first computer-based speech synthesis systems were created in the late 1950s and the first complete text-to-speech system was completed in <a href="/wiki/1968.html" title="1968">1968</a>.</p>
<p>In <a href="/wiki/1961.html" title="1961">1961</a>, physicist <a href="/wiki/John_Larry_Kelly%2C_Jr.html" title="John Larry Kelly, Jr">John Larry Kelly, Jr</a> used an <a href="/wiki/IBM_704.html" title="IBM 704">IBM 704</a> computer to synthesize speech, an event which would appear to be one of the most famous moments in the history of <a href="/wiki/Bell_Labs.html" title="Bell Labs">Bell Labs</a>. Kelly's voice recorder synthesizer (vocoder) recreated the song <i><a href="/wiki/Daisy_Bell.html" title="Daisy Bell">Daisy Bell</a></i>, with musical accompaniment from <a href="/wiki/Max_Mathews.html" title="Max Mathews">Max Mathews</a>. Coincidentally, <a href="/wiki/Arthur_C._Clarke.html" title="Arthur C. Clarke">Arthur C. Clarke</a> was visiting his friend and colleague <a  class="new" title="John Pierce">John Pierce</a> at the <a  class="new" title="Bell Labs Murray Hill facility">Bell Labs Murray Hill facility</a>. Clarke was so impressed by the demonstration that he used it in the climactic scene of his novel and screenplay for <i><a  title="2001: A Space Odyssey">2001: A Space Odyssey</a></i>,<sup id="_ref-Arthur_C_Clarke_0" class="reference"><a  title="">[1]</a></sup> where the <i><a href="/wiki/HAL_9000.html" title="HAL 9000">HAL 9000</a></i> computer sings the same song as it is being put to sleep by astronaut <a href="/wiki/Dave_Bowman.html" title="Dave Bowman">Dave Bowman</a>. <sup id="_ref-bell_labs_hal_0" class="reference"><a  title="">[2]</a></sup></p>
<p>The technologies used to synthetize speech have greatly evolved since then. See the <a  title="">External links section</a> below for state-of-the-art commercial and free text-to-speech systems.</p>
<p><b>References</b>:</p>
<ul>
<li><a  class="external text" title="http://www.cs.indiana.edu/rhythmsp/ASA/Contents.html">Dennis Klatt's History of Speech Synthesis</a></li>
<li><a  class="external text" title="http://www.acoustics.hut.fi/~slemmett/dippa/chap2.html">History and Development of Speech Synthesis (Helsinki University of Technology)</a></li>
</ul>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Synthesizer technologies">edit</a>]</div>
<p><a name="Synthesizer_technologies" id="Synthesizer_technologies"></a></p>
<h2>Synthesizer technologies</h2>
<p>The two characteristics used to describe the quality of a speech synthesis system are <i>naturalness</i> and <i>intelligibility</i>. The <i>naturalness</i> of a speech synthesizer refers to how much the output sounds like the speech of a real person. Its <i>intelligibility</i> refers to how easily the output can be understood. The ideal speech synthesizer is both natural and intelligible, and each of the different synthesis technologies try to maximize both of these characteristics. Some of the technologies are better at naturalness or intelligibility and the goals of a synthesis system will often determine what approach is used. There are two main technologies used for the generating synthetic speech waveforms (<b>concatenative synthesis</b> and <b><a href="/wiki/Formant.html" title="Formant">formant</a> synthesis</b>), as well as a few others.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Concatenative synthesis">edit</a>]</div>
<p><a name="Concatenative_synthesis" id="Concatenative_synthesis"></a></p>
<h3>Concatenative synthesis</h3>
<p>Concatenative synthesis is based on the concatenation (or stringing together) of segments of recorded speech. Generally, concatenative synthesis gives the most natural sounding synthesized speech. However, differences between natural variations in speech and the nature of the automated techniques for segmenting the waveforms sometimes result in audible glitches in the output, detracting from the naturalness of the synthesized speech. There are three main subtypes of concatenative synthesis.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Unit selection synthesis">edit</a>]</div>
<p><a name="Unit_selection_synthesis" id="Unit_selection_synthesis"></a></p>
<h4>Unit selection synthesis</h4>
<p>Unit selection synthesis uses large speech <a href="/wiki/Database.html" title="Database">databases</a> (more than one hour of recorded speech). During database creation, each recorded <a href="/wiki/Utterance.html" title="Utterance">utterance</a> is segmented into some or all of the following: individual <a href="/wiki/Phone.html" title="Phone">phones</a>, <a href="/wiki/Syllable.html" title="Syllable">syllables</a>, <a href="/wiki/Morpheme.html" title="Morpheme">morphemes</a>, <a href="/wiki/Word.html" title="Word">words</a>, <a href="/wiki/Phrase.html" title="Phrase">phrases</a>, and <a href="/wiki/Sentence_%28linguistics%29.html" title="Sentence (linguistics)">sentences</a>. Typically, the division into segments is done using a specially modified <a href="/wiki/Speech_recognition.html" title="Speech recognition">speech recognizer</a> set to a "forced alignment" mode with some hand correction afterward, using visual representations such as the <a href="/wiki/Waveform.html" title="Waveform">waveform</a> and <a href="/wiki/Spectrogram.html" title="Spectrogram">spectrogram</a>. An <a href="/wiki/Index_%28database%29.html" title="Index (database)">index</a> of the units in the speech database is then created based on the segmentation and acoustic parameters like the <a href="/wiki/Fundamental_frequency.html" title="Fundamental frequency">fundamental frequency</a> (<a href="/wiki/Pitch_%28music%29.html" title="Pitch (music)">pitch</a>), duration, position in the syllable, and neighboring phones. At <a href="/wiki/Runtime.html" title="Runtime">runtime</a>, the desired target utterance is created by determining the best chain of candidate units from the database (unit selection). This process is typically achieved using a specially weighted <a href="/wiki/Decision_tree.html" title="Decision tree">decision tree</a>.</p>
<p>Unit selection gives the greatest naturalness due to the fact that it does not apply a large amount of <a href="/wiki/Digital_signal_processing.html" title="Digital signal processing">digital signal processing</a> to the recorded speech, which often makes recorded speech sound less natural, although some systems may use a small amount of signal processing at the point of concatenation to smooth the waveform. In fact, output from the best unit-selection systems is often indistinguishable from real human voices, especially in contexts for which the TTS system has been tuned. However, maximum naturalness often requires unit-selection speech databases to be very large, in some systems ranging into the gigabytes of recorded data and numbering into the dozens of hours of recorded speech.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Diphone synthesis">edit</a>]</div>
<p><a name="Diphone_synthesis" id="Diphone_synthesis"></a></p>
<h4>Diphone synthesis</h4>
<p>Diphone synthesis uses a minimal speech database containing all the <a href="/wiki/Diphone.html" title="Diphone">Diphones</a> (sound-to-sound transitions) occurring in a given language. The number of diphones depends on the <a href="/wiki/Phonotactics.html" title="Phonotactics">phonotactics</a> of the language: Spanish has about 800 diphones, German about 2500. In diphone synthesis, only one example of each diphone is contained in the speech database. At runtime, the target <a href="/wiki/Prosody.html" title="Prosody">prosody</a> of a sentence is superimposed on these minimal units by means of <a href="/wiki/Digital_signal_processing.html" title="Digital signal processing">digital signal processing</a> techniques such as <a href="/wiki/Linear_predictive_coding.html" title="Linear predictive coding">Linear predictive coding</a>, <a href="/wiki/PSOLA.html" title="PSOLA">PSOLA</a> or <a  class="new" title="MBROLA">MBROLA</a>.</p>
<p>The quality of the resulting speech is generally not as good as that from unit selection but more natural-sounding than the output of formant synthesizers. Diphone synthesis suffers from the sonic glitches of concatenative synthesis and the robotic-sounding nature of formant synthesis, and has few of the advantages of either approach other than small size. As such, its use in commercial applications is declining, although it continues to be used in research because there are a number of freely available implementations.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Domain-specific synthesis">edit</a>]</div>
<p><a name="Domain-specific_synthesis" id="Domain-specific_synthesis"></a></p>
<h4>Domain-specific synthesis</h4>
<p>Domain-specific synthesis concatenates pre-recorded words and phrases to create complete utterances. It is used in applications where the variety of texts the system will output is limited to a particular domain, like transit schedule announcements or weather reports.</p>
<p>This technology is very simple to implement, and has been in commercial use for a long time, in <a href="/wiki/Gadget.html" title="Gadget">gadgets</a> like talking clocks and calculators. The naturalness of these systems can potentially be very high because the variety of sentence types is limited and closely matches the prosody and intonation of the original recordings. However, because these systems are limited by the words and phrases in its database, they are not general-purpose and can only synthesize the combinations of words and phrases they have been pre-programmed with.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Formant synthesis">edit</a>]</div>
<p><a name="Formant_synthesis" id="Formant_synthesis"></a></p>
<h3>Formant synthesis</h3>
<p><a href="/wiki/Formant.html" title="Formant">Formant</a> synthesis does not use any human speech samples at runtime. Instead, the output synthesized speech is created using an acoustic model. Parameters such as <a href="/wiki/Fundamental_frequency.html" title="Fundamental frequency">fundamental frequency</a>, <a href="/wiki/Phonation.html" title="Phonation">voicing</a>, and <a href="/wiki/Noise.html" title="Noise">noise</a> levels are varied over time to create a <a href="/wiki/Waveform.html" title="Waveform">waveform</a> of artificial speech. This method is sometimes called <b>rule-based synthesis</b>, but some argue <sup title="The text in the vicinity of this tag needs citation." class="noprint">[<a  title="Wikipedia:Citing sources"><i>citation&#160;needed</i></a>]</sup> that because many concatenative systems use rule-based components for some parts of the system, like the front end, the term is not specific enough.</p>
<p>Many systems based on formant synthesis technology generate artificial, robotic-sounding speech, and the output would never be mistaken for the speech of a real human. However, maximum naturalness is not always the goal of a speech synthesis system, and formant synthesis systems have some advantages over concatenative systems.</p>
<p>Formant synthesized speech can be very reliably intelligible, even at very high speeds, avoiding the acoustic glitches that can often plague concatenative systems. High speed synthesized speech is often used by the visually impaired for quickly navigating computers using a <a href="/wiki/Screen_reader.html" title="Screen reader">screen reader</a>. Second, formant synthesizers are often smaller programs than concatenative systems because they do not have a database of speech samples. They can thus be used in <a href="/wiki/Embedded_system.html" title="Embedded system">embedded computing</a> situations where memory space and processor power are often scarce. Last, because formant-based systems have total control over all aspects of the output speech, a wide variety of prosody or <a href="/wiki/Intonation.html" title="Intonation">intonation</a> can be output, conveying not just questions and statements, but a variety of emotions and tones of voice.</p>
<p>Examples of non-real-time but highly accurate intonation control in formant synthesis include the work done in the late 1970s for the <a href="/wiki/Texas_Instruments.html" title="Texas Instruments">Texas Instruments</a> toy <a href="/wiki/Speak_%26_Spell_%28game%29.html" title="Speak &amp; Spell (game)">Speak &amp; Spell</a>, and the early 1980s <a href="/wiki/SEGA.html" title="SEGA">SEGA</a> coin-op <a href="/wiki/Video_arcade.html" title="Video arcade">arcade</a> machines: <a href="/wiki/Astro_Blaster.html" title="Astro Blaster">Astro Blaster</a>, <a  class="new" title="Zektor">Zektor</a>, <a href="/wiki/Space_Fury.html" title="Space Fury">Space Fury</a>, and <a href="/wiki/Star_Trek_%28arcade_game%29.html" title="Star Trek (arcade game)">Star Trek: Strategic Operations Simulator</a>. Creating proper intonation for these various projects was a painstaking process, and one that still has yet to be matched by real-time text-to-speech interfaces.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Articulatory synthesis">edit</a>]</div>
<p><a name="Articulatory_synthesis" id="Articulatory_synthesis"></a></p>
<h3>Articulatory synthesis</h3>
<p>Articulatory synthesis has been a synthesis method mostly of academic interest until recently. It is based on computational models of the human <a href="/wiki/Vocal_tract.html" title="Vocal tract">vocal tract</a> and the articulation processes occurring there. Few of these models are currently sufficiently advanced or computationally efficient to be used in commercial speech synthesis systems. A notable exception is the <a href="/wiki/NeXT.html" title="NeXT">NeXT</a>-based system originally developed and marketed by <a  class="new" title="Trillium Sound Research Inc">Trillium Sound Research Inc</a>, a <a href="/wiki/Calgary.html" title="Calgary">Calgary</a>, <a href="/wiki/Alberta.html" title="Alberta">Alberta</a>, <a href="/wiki/Canada.html" title="Canada">Canada</a>-based software spin-off company from the <a href="/wiki/University_of_Calgary.html" title="University of Calgary">University of Calgary</a> where much of the original research was conducted. Following the demise of the various incarnations of <a href="/wiki/NeXT.html" title="NeXT">NeXT</a> (started by <a href="/wiki/Steve_Jobs.html" title="Steve Jobs">Steve Jobs</a> in the late 1980s and merged with <a href="/wiki/Apple_Computer.html" title="Apple Computer">Apple</a> in <a href="/wiki/1997.html" title="1997">1997</a>), the Trillium software was put out under a <a href="/wiki/General_Public_Licence.html" title="General Public Licence">General Public Licence</a> (GPL) -- see <a  class="external text" title="http://www.gnu.org">the GNU web site</a>, with work continuing as <b>gnuspeech</b> -- a <a href="/wiki/GNU.html" title="GNU">GNU</a> project. The original <a href="/wiki/NeXT.html" title="NeXT">NeXT</a> software and recent ports of major portions of that software to both <a  class="new" title="Mac OS/X">Mac OS/X</a> and <a href="/wiki/GNU/Linux.html" title="GNU/Linux">GNU/Linux</a> <a href="/wiki/GNUstep.html" title="GNUstep">GNUstep</a> are available on the <a  class="external text" title="http://savannah.gnu.org/projects/gnuspeech">GNU savannah site</a> along with access to on-line manuals and papers relevant to the theoretical underpinnings of the work. The system, first marketed in <a href="/wiki/1994.html" title="1994">1994</a>, provides full articulatory-based text-to-speech conversion using a waveguide or transmission-line analog of the human oral and nasal tracts controlled by Carré's <b>Distinctive Region Model</b> that is, in turn, based on work by <a href="/wiki/Gunnar_Fant.html" title="Gunnar Fant">Gunnar Fant</a> and others at the <a  class="new" title="Stockholm Speech Technology Lab">Stockholm Speech Technology Lab</a> of the <a href="/wiki/Royal_Institute_of_Technology.html" title="Royal Institute of Technology">Royal Institute of Technology</a> on formant sensitivity analysis. This work showed that the formants in a resonant tube can be controlled by just eight parameters that correspond closely to the naturally available articulators in the human vocal tract. The system embodies a full pronouncing dictionary look-up together with context sensitive rules for posture concatenation and parameter generation as well as models of rhythm and intonation derived from linguistic/phonological research.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Hybrid synthesis">edit</a>]</div>
<p><a name="Hybrid_synthesis" id="Hybrid_synthesis"></a></p>
<h3>Hybrid synthesis</h3>
<p>Hybrid synthesis marries aspects of formant and concatenative synthesis to minimize the acoustic glitches when speech segments are concatenated.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: HMM-based synthesis">edit</a>]</div>
<p><a name="HMM-based_synthesis" id="HMM-based_synthesis"></a></p>
<h3>HMM-based synthesis</h3>
<p>HMM-based synthesis is a synthesis method based on <a href="/wiki/Hidden_Markov_Model.html" title="Hidden Markov Model">Hidden Markov Models</a> (HMMs). In this system, speech <a href="/wiki/Frequency_spectrum.html" title="Frequency spectrum">frequency spectrum</a> (<a href="/wiki/Vocal_tract.html" title="Vocal tract">vocal tract</a>), <a href="/wiki/Fundamental_frequency.html" title="Fundamental frequency">fundamental frequency</a> (vocal source), and duration (<a href="/wiki/Prosody.html" title="Prosody">prosody</a>) are modeled simultaneously by HMMs. Speech <a href="/wiki/Waveforms.html" title="Waveforms">waveforms</a> are generated from HMMs themselves based on <a href="/wiki/Maximum_likelihood.html" title="Maximum likelihood">Maximum likelihood</a> criterion.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Front-end challenges">edit</a>]</div>
<p><a name="Front-end_challenges" id="Front-end_challenges"></a></p>
<h2>Front-end challenges</h2>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Text normalization challenges">edit</a>]</div>
<p><a name="Text_normalization_challenges" id="Text_normalization_challenges"></a></p>
<h3>Text normalization challenges</h3>
<p>The process of normalizing text is rarely straightforward. Texts are full of <a href="/wiki/Homograph.html" title="Homograph">homographs</a>, <a href="/wiki/Number.html" title="Number">numbers</a> and <a href="/wiki/Abbreviation.html" title="Abbreviation">abbreviations</a> that all ultimately require expansion into a phonetic representation.</p>
<p>There are many spellings in English which are pronounced differently based on context. Some examples:</p>
<ul>
<li><b>project</b>: My latest project is to learn how to better project my voice.</li>
<li><b>bow</b>: The girl with the bow in her hair was told to bow deeply when greeting her superiors.</li>
<li><b>bass</b>: My hobbies are bass fishing, and playing the bass.</li>
</ul>
<p>Most TTS systems do not generate semantic representations of their input texts, as processes for doing so are not reliable, well understood, or computationally effective. As a result, various <a href="/wiki/Heuristic.html" title="Heuristic">heuristic</a> techniques are used to guess the proper way to disambiguate homographs, like looking at neighboring words and using statistics about frequency of occurrence.</p>
<p>Deciding how to convert numbers is another problem TTS systems have to address. It is a simple programming challenge to convert a number into words, like 1325 becoming "one thousand three hundred twenty-five." However, numbers occur in many different contexts in texts, and 1325 should probably be read as "thirteen twenty-five" when part of an address (1325 Main St.), and as "one three two five" if it is the last four digits of a social security number. Often a TTS system can infer how to expand a number based on surrounding words, numbers, and punctuation, and sometimes the systems provide a way to specify the type of context if it is ambiguous.</p>
<p>Similarly, abbreviations such as "<b>etc.</b>" are easily rendered as "et cetera," but often abbreviations can be ambiguous. For example, the abbreviation "<b>in</b>" in the following example: "Yesterday it rained 3 in. Take 1 out, then put 3 in.". "<b>St.</b>" can also be ambiguous: "St. John St." TTS systems with intelligent front ends can make educated guesses about how to deal with ambiguous abbreviations, while others do the same thing in all cases, resulting in nonsensical but sometimes comical outputs: "Yesterday it rained three in." or "Take one out, then put three inches."</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Text-to-phoneme challenges">edit</a>]</div>
<p><a name="Text-to-phoneme_challenges" id="Text-to-phoneme_challenges"></a></p>
<h3>Text-to-phoneme challenges</h3>
<p>Speech synthesis systems use two basic approaches to determine the pronunciation of a word based on its spelling, a process which is often called text-to-phoneme or grapheme-to-phoneme conversion, as <a href="/wiki/Phoneme.html" title="Phoneme">phoneme</a> is the term used by <a href="/wiki/Linguist.html" title="Linguist">linguists</a> to describe distinctive sounds in a language.</p>
<p>The simplest approach to text-to-phoneme conversion is the <b>dictionary-based</b> approach, where a large dictionary containing all the words of a language and their correct pronunciation is stored by the program. Determining the correct pronunciation of each word is a matter of looking up each word in the dictionary and replacing the spelling with the pronunciation specified in the dictionary.</p>
<p>The other approach used for text-to-phoneme conversion is the <b>rule-based</b> approach, where rules for the pronunciations of words are applied to words to work out their pronunciations based on their spellings. This is similar to the "sounding out," or <a href="/wiki/Synthetic_phonics.html" title="Synthetic phonics">synthetic phonics</a>, approach to learning reading.</p>
<p>Each approach has advantages and drawbacks. The dictionary-based approach has the advantages of being quick and accurate, but it completely fails if it is given a word which is not in its dictionary, and, as dictionary size grows, so too does the memory space requirements of the synthesis system. On the other hand, the rule-based approach works on any input, but the complexity of the rules grows substantially as it takes into account irregular spellings or pronunciations. (Consider that the word "of" is both very common in English—and also the only word in the language in which the letter "f" is pronounced [v].) As a result, nearly all speech synthesis systems use a combination of both approaches.</p>
<p>Some languages, like <a href="/wiki/Spanish_language.html" title="Spanish language">Spanish</a>, have a very regular writing system, and the prediction of the pronunciation of words based on their spelling is quite successful. Speech synthesis systems for languages like this often use the rule-based method as the core means of text-to-phoneme conversion, resorting to dictionaries only for those few words, like foreign names and borrowings, whose pronunciation is not obvious from the spelling. On the other hand, speech synthesis for languages like <a href="/wiki/English_language.html" title="English language">English</a>, which have extremely irregular spelling systems, often rely mostly on dictionaries and use rule-based methods only for unusual words or names that aren't in the dictionary.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Speech synthesis markup languages">edit</a>]</div>
<p><a name="Speech_synthesis_markup_languages" id="Speech_synthesis_markup_languages"></a></p>
<h2>Speech synthesis markup languages</h2>
<p>A number of <a href="/wiki/Markup_language.html" title="Markup language">markup languages</a> have been established for rendition of text as speech in an <a href="/wiki/XML.html" title="XML">XML</a> compliant format, the most recent being <a href="/wiki/SSML.html" title="SSML">SSML</a> proposed by the <a href="/wiki/W3C.html" title="W3C">W3C</a> which is in draft status at the time of this writing. Older speech synthesis markup languages include <a  class="new" title="SABLE">SABLE</a> and <a href="/wiki/JSML.html" title="JSML">JSML</a>. Although each of these was proposed as a new standard, still none of them has been widely adopted.</p>
<p>A subset of the <a href="/wiki/Cascading_Style_Sheets.html" title="Cascading Style Sheets">Cascading Style Sheets</a> 2 specification includes <a  class="new" title="Aural Cascading Style Sheets">Aural Cascading Style Sheets</a>.</p>
<p>Speech synthesis markup languages should be distinguished from dialogue markup languages such as <a href="/wiki/VoiceXML.html" title="VoiceXML">VoiceXML</a>, which includes, in addition to text-to-speech markup, tags related to speech recognition, dialogue management and touchtone dialing.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Operating systems featuring speech synthesis">edit</a>]</div>
<p><a name="Operating_systems_featuring_speech_synthesis" id="Operating_systems_featuring_speech_synthesis"></a></p>
<h2>Operating systems featuring speech synthesis</h2>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Apple">edit</a>]</div>
<p><a name="Apple" id="Apple"></a></p>
<h3><a href="/wiki/Apple_Computer.html" title="Apple Computer">Apple</a></h3>
<div class="boilerplate seealso">
<dl>
<dd><i>For more details on this topic, see <a href="/wiki/Apple_PlainTalk.html" title="Apple PlainTalk">Apple PlainTalk</a>.</i></dd>
</dl>
</div>
<p>The first speech system integrated into an OS was <a href="/wiki/Apple_Computer.html" title="Apple Computer">Apple Computer</a>'s <a href="/wiki/Apple_PlainTalk#The_original_MacInTalk.html" title="Apple PlainTalk">Macintalk</a> in <a href="/wiki/1984.html" title="1984">1984</a>. <a href="/wiki/Apple_Computer.html" title="Apple Computer">Apple</a> was one of the first manufacturers to include speech as part of a mainstream operating system. Historically, during most of the early 90s, <a href="/wiki/Apple_Computer.html" title="Apple Computer">Apple</a> voices were synthetic in nature. However, more recently, <a href="/wiki/Apple_Computer.html" title="Apple Computer">Apple</a> has also added a couple of sample-based voices, namely Vicki and Bruce - named for the <a  class="new" title="UCLA linguistics">UCLA linguistics</a> professor and graduate student who provided the models for the synthesis. Starting as a curiosity program to catch people's interest (in its earliest releases <a href="/wiki/Apple_Computer.html" title="Apple Computer">Apple</a> did not even support it directly), the speech system of <a href="/wiki/Macintosh_%28computer%29.html" title="Macintosh (computer)">Macintosh</a> eventually evolved to a fully-supported program for people with vision problems.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: AmigaOS">edit</a>]</div>
<p><a name="AmigaOS" id="AmigaOS"></a></p>
<h3><a href="/wiki/AmigaOS.html" title="AmigaOS">AmigaOS</a></h3>
<p>The second OS on the market with advanced speech synthesis capabilities was <a href="/wiki/AmigaOS.html" title="AmigaOS">AmigaOS</a> in <a href="/wiki/1985.html" title="1985">1985</a>. The voice synthesis was licensed by <a href="/wiki/Commodore.html" title="Commodore">Commodore</a> from a third-party software house (<a  class="new" title="Don't Ask Software">Don't Ask Software</a>, now <a  class="new" title="Softvoice, Inc.">Softvoice, Inc.</a>) and it featured a complete system of human voice emulation, complete with both male and female voices and "stress" indicator markers, made possible by advanced features of the <a href="/wiki/Amiga.html" title="Amiga">Amiga</a> hardware audio <a href="/wiki/Chipset.html" title="Chipset">chipset</a>. It was divided into a narrator device and a translator library. Amiga <a href="/wiki/AmigaOS#Speech_Synthesis.html" title="AmigaOS">Speak Handler</a> featured also a TTS (text-to-speech) translator, that is, a Text-to-Phoneme System using the <a  class="new" title="ARPAbet">ARPAbet</a> TTS phoneme system. <a href="/wiki/AmigaOS.html" title="AmigaOS">AmigaOS</a> considered speech synthesis as a virtual hardware device, so the user could even re-direct console output to it as a quite normal peripheral such as monitor or printer. Some programs into Amiga, such as word processors, made extensive use of the speech system.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Windows">edit</a>]</div>
<p><a name="Windows" id="Windows"></a></p>
<h3><a href="/wiki/Microsoft_Windows.html" title="Microsoft Windows">Windows</a></h3>
<p>Modern <a href="/wiki/Microsoft_Windows.html" title="Microsoft Windows">Windows</a> systems use <a href="/wiki/Speech_Application_Programming_Interface#SAPI_1-4_API_family.html" title="Speech Application Programming Interface">SAPI4</a> and <a href="/wiki/Speech_Application_Programming_Interface#SAPI_5_API_family.html" title="Speech Application Programming Interface">SAPI5</a> based speech systems, that include also a <a  class="new" title="Speech Recognition Engine">Speech Recognition Engine</a> (SRE). SAPI 4.0 was available on <a href="/wiki/Microsoft.html" title="Microsoft">Microsoft</a> based OSes like Windows 9x.</p>
<p>Programs, such as <a href="/wiki/MIRC.html" title="MIRC">mIRC</a>, made widely use of SAPI 4.0 (now 5.0) capabilities. <a href="/wiki/Windows_XP.html" title="Windows XP">Windows XP</a> features a speech synthesis program called <a href="/wiki/Microsoft_Narrator.html" title="Microsoft Narrator">Narrator</a> directly available to users. All <a href="/wiki/Microsoft_Windows.html" title="Microsoft Windows">Windows</a> compatible programs such as <a href="/wiki/Notepad.html" title="Notepad">Notepad</a>, <a href="/wiki/Office.html" title="Office">Office</a> or even <a href="/wiki/Adobe_Acrobat.html" title="Adobe Acrobat">Adobe Acrobat</a> could make use of speech synthesis features which are available through menu choices once installed into the system. Speech synthesis in Windows is a useful feature to help users with vision impairment.</p>
<p>An example of how SAPI 5 has enabled a software product to combine Microsoft technologies into an interactive desktop experience is <a href="/wiki/Talking_desktop.html" title="Talking desktop">Talking desktop</a>. This software combines speech recognition with any SAPI 5 compatible text-to-speech voices.</p>
<p><a href="/wiki/Microsoft_Speech_Server.html" title="Microsoft Speech Server">Microsoft Speech Server</a> is also worth mentioning. MSS is a complete package for voice synthesis and recognition, and would be useful for commercial or institutional application in a call center based on Windows.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: GNU/Linux">edit</a>]</div>
<p><a name="GNU.2FLinux" id="GNU.2FLinux"></a></p>
<h3><a href="/wiki/GNU/Linux.html" title="GNU/Linux">GNU/Linux</a></h3>
<p><a href="/wiki/GNU/Linux.html" title="GNU/Linux">GNU/Linux</a> speech synthesis systems are various and are all <a href="/wiki/Open-source.html" title="Open-source">open-source</a> programs, such as <a href="/wiki/Festival_Speech_Synthesis_System.html" title="Festival Speech Synthesis System">Festival</a>, from the <a href="/wiki/University_of_Edinburgh.html" title="University of Edinburgh">University of Edinburgh</a>, or <a  class="external text" title="http://www.gnu.org/software/gnuspeech/">gnuspeech</a>, from the <a href="/wiki/Free_Software_Foundation.html" title="Free Software Foundation">Free Software Foundation</a>.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Texas Instruments TI-99/4 and TI-99/4A">edit</a>]</div>
<p><a name="Texas_Instruments_TI-99.2F4_and_TI-99.2F4A" id="Texas_Instruments_TI-99.2F4_and_TI-99.2F4A"></a></p>
<h3><a href="/wiki/Texas_Instruments.html" title="Texas Instruments">Texas Instruments</a> <a href="/wiki/TI-99.html" title="TI-99">TI-99/4 and TI-99/4A</a></h3>
<p>Notable for its antiquity and with TI being a pioneer of speech synthesis (<a href="/wiki/Speak_%26_Spell_%28toy%29.html" title="Speak &amp; Spell (toy)">Speak &amp; Spell</a>), the 1979 and 1981 (respectively) Texas Instruments home computers were capable of text-to-phoneme (using the optional speech synthesizer and Terminal Emulator cartridge) or reciting complete words and phrases (text-to-dictionary) (using the optional speech sythesizer and TI Extended BASIC or several other software cartridges).</p>
<p>In TI Extended BASIC, the CALL SAY statement would be used. For example, CALL SAY("I AM A TEXAS INSTRUMENTS T I 99 4 A HOME COMPUTER") would cause it to speak out its identity with a clear Texan accent. In this text-to-dictionary system, unknown words would be spelled. In text-to-phoneme mode (Terminal Emulator), quality was substantially reduced though the speech synthesizer would attempt to speak any text sent to it.</p>
<p>When complete words and phrases were used, a Texan accent was usually obvious. In many video games, sarcasm at mistakes was also obvious ("Did you mean to do that?" by a mocking female voice in Alpiner).</p>
<p>Like the rest of the early computing platforms in this list, the TI-99/4 (1979) and TI-99/4A (1981) were powered by a 16-bit processor, remarkable for a computer of this vintage.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Third party systems">edit</a>]</div>
<p><a name="Third_party_systems" id="Third_party_systems"></a></p>
<h3>Third party systems</h3>
<p>Other third party systems which could be included into modern <a href="/wiki/Operating_System.html" title="Operating System">Operating Systems</a> are (apart from SAPI) <a href="/wiki/Lernout_%26_Hauspie.html" title="Lernout &amp; Hauspie">Lernout &amp; Hauspie</a> (LH) <a  class="new" title="TTS 3000">TTS 3000</a>, <a  class="new" title="1st Read It Aloud!">Rich Speech</a>, <a  class="new" title="Total Speech">Softplicity Development</a>, <a  class="new" title="PCVoz">EzHermatic</a>, <a  class="new" title="TextAloud">NextUp Technologies</a>, <a  class="new" title="Read Genius">Vioio Software Inc</a> and <a  class="new" title="Speech RealSpeak">Speech RealSpeak</a>, <a href="/wiki/IBM_ViaVoice.html" title="IBM ViaVoice">IBM ViaVoice</a> and <a  class="new" title="Dolphin Orpheus">Dolphin Orpheus</a> <a  class="external autonumber" title="http://www.dolphincomputeraccess.com/products/hal.htm">[1]</a>.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: References">edit</a>]</div>
<p><a name="References" id="References"></a></p>
<h2>References</h2>
<div class="references-small">
<ol class="references">
<li id="_note-Arthur_C_Clarke"><b><a  title="">^</a></b> <a  class="external text" title="http://www.lsi.usp.br/~rbianchi/clarke/ACC.Biography.html">Arthur C. Clarke online Biography</a></li>
<li id="_note-bell_labs_hal"><b><a  title="">^</a></b> <a  class="external text" title="http://www.bell-labs.com/news/1997/march/5/2.html">Bell Labs: Where "HAL" First Spoke (Bell Labs Speech Synthesis website)</a></li>
</ol>
</div>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: See also">edit</a>]</div>
<p><a name="See_also" id="See_also"></a></p>
<h2>See also</h2>
<ul>
<li><a href="/wiki/Speech_processing.html" title="Speech processing">Speech processing</a>;</li>
<li><a href="/wiki/Speech_recognition.html" title="Speech recognition">Speech recognition</a>;</li>
<li><a href="/wiki/Speech_to_text.html" title="Speech to text">Speech to text</a> (<a href="/wiki/Dictation.html" title="Dictation">dictation</a>);</li>
<li><a href="/wiki/Natural_language_processing.html" title="Natural language processing">Natural language processing</a>;</li>
<li><a href="/wiki/Sonification.html" title="Sonification">Sonification</a> (the use of non-speech audio to convey information);</li>
<li><a href="/wiki/Software_Automatic_Mouth.html" title="Software Automatic Mouth">Software Automatic Mouth</a>;</li>
<li><a href="/wiki/Apple_PlainTalk.html" title="Apple PlainTalk">Apple PlainTalk</a>;</li>
<li><a href="/wiki/FreeTTS.html" title="FreeTTS">FreeTTS</a>;</li>
<li><a href="/wiki/Talking_desktop.html" title="Talking desktop">Talking desktop</a> (how speech can enhance computer feedback);</li>
<li><a href="/wiki/Lernout_%26_Hauspie.html" title="Lernout &amp; Hauspie">Lernout &amp; Hauspie</a>;</li>
<li><a href="/wiki/List_of_songs_about_robots.html" title="List of songs about robots">List of songs about robots</a>.</li>
</ul>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: External links">edit</a>]</div>
<p><a name="External_links" id="External_links"></a></p>
<h2>External links</h2>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Misc">edit</a>]</div>
<p><a name="Misc" id="Misc"></a></p>
<h3>Misc</h3>
<ul>
<li><a  class="external text" title="http://www.tmaa.com/tts/comparison_USEng_highres.htm">Samples</a> of commercial TTS systems.</li>
<li><a  class="external text" title="http://www.chipspeaking.com">Free</a> Speech Synthesis system designed for the vocally impaired, with links to other speech related assistive technologies and resources for PALS.</li>
<li><a  class="external text" title="http://www.speech.cs.cmu.edu/comp.speech/">comp.speech Frequently Asked Questions</a></li>
<li><a  class="external text" title="http://www.vintagecomputermusic.com/">Audio of 1962 Bell Laboratories computer speech demonstration</a></li>
<li><a  class="external text" title="http://www.disc2.dk/tools/SGsurvey.html#narr">A survey on speech synthesis and commercial speech synthesis systems</a></li>
<li><a  class="external text" title="http://prt-i61.fernuni-hagen.de/~bischoff/radiopedia/index_en.html">Pediaphon generates MP3 audio files and podcasts from Wikipedia articles.</a></li>
</ul>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Freely available TTS systems">edit</a>]</div>
<p><a name="Freely_available_TTS_systems" id="Freely_available_TTS_systems"></a></p>
<h3>Freely available TTS systems</h3>
<ul>
<li><a  class="external text" title="http://www.asel.udel.edu/speech/users.html">ModelTalker</a> is a TTS system designed with the needs of <a href="/wiki/Augmentative_and_alternative_communication.html" title="Augmentative and alternative communication">AAC</a> users in mind, though anyone can currently download the system and create their own synthesized voice; registration is required.</li>
<li><a  class="external text" title="http://feed2podcast.com/">Feed2Podcast.com</a> is a service that turns any RSS feed into an audio file or podcast, synthesizer for English, and potentially other languages</li>
<li><a  class="external text" title="http://www.microsoft.com/msagent/downloads/user.asp#tts">Sapi 4.0</a> supports older Lernout &amp; Hauspie TTS3000 engines for US English, UK English, Dutch, French, German, Italian, Japanese, Korean, Russian, Spanish, and Brazilian Portuguese.</li>
<li><a  class="external text" title="http://www.cstr.ed.ac.uk/projects/festival/">Festival</a> is a freely available complete diphone concatenation and unit selection TTS system for British and American English, Spanish and Welsh.</li>
<li><a  class="external text" title="http://www.speech.cs.cmu.edu/flite/">Flite</a> (Festival-lite) is a smaller, faster alternative version of <a href="/wiki/Festival_Speech_Synthesis_System.html" title="Festival Speech Synthesis System">Festival</a> designed for embedded systems and high volume servers.</li>
<li><a  class="external text" title="http://freetts.sourceforge.net/docs/index.php">FreeTTS</a> written entirely in <a href="/wiki/Java_programming_language.html" title="Java programming language">Java</a>, based on <a href="/wiki/Flite.html" title="Flite">Flite</a>.</li>
<li><a  class="external text" title="http://tcts.fpms.ac.be/synthesis/mbrola.html">MBROLA</a> is a freely available diphone concatenation system for about 25 languages (back end only).</li>
<li><a  class="external text" title="http://www.gnu.org/software/gnuspeech/">Gnuspeech</a> is an extensible, text-to-speech package, based on real-time, articulatory, speech-synthesis-by-rules.</li>
<li><a  class="external text" title="http://epos.ure.cas.cz/">Epos</a> is a rule-driven TTS system primarily designed to serve as a research tool. It supports Czech and Slovak.</li>
<li><a  class="external text" title="http://hts.ics.nitech.ac.jp/voicedemos.html">HTS voices</a> are freely available HMM-based speech synthesis voices for the Festival. You can construct your own HTS voice using small amount of speech data (about 30 minutes) using training tools distributed at <a  class="external text" title="http://hts.ics.nitech.ac.jp/">HTS website</a>.</li>
<li><a  class="external text" title="http://minchu.ee.iisc.ernet.in/new/biolab/try.htm">Indian TTS Systems</a> Tamil and Kannada Text to Speech Synthesis Systems are developed by Medical Intelligence and Language Engineering Laboratory, Indian Institute of Science, Bangalore.</li>
<li><a  class="external autonumber" title="http://espeak.sourceforge.net/espeak">[2]</a> is a software speech synthesizer for English, and potentially other languages.</li>
<li><a  class="external text" title="http://www.newscasta.com/">www.NewsCasta.com</a> is a service that turns RSS feeds and Google News queries into a podcast using Microsoft's TTS SAPI.</li>
</ul>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Commercially available TTS systems">edit</a>]</div>
<p><a name="Commercially_available_TTS_systems" id="Commercially_available_TTS_systems"></a></p>
<h3>Commercially available TTS systems</h3>
<ul>
<li><a  class="external text" title="http://www.acapela-group.com">Acapela Group</a> with several TTS systems (unit selection and diphone synthesis) that support 19 languages and 50 voices (<a  class="external text" title="http://demo.acapela-group.com/">Unit Selection Multilingual demo</a>)</li>
<li><a  class="external text" title="http://www.alfanum.co.yu/">AlfaNum govorne tehnologije</a> Serbian, Croatian and Macedonian TTS and ASR engines.</li>
<li><a  class="external text" title="http://www.atip.de/german/technologie/tts/proseronline.htm">ATIP's TTS Voices</a> for German (also with French and Turkish accents) and English.</li>
<li><a  class="external text" title="http://www.naturalvoices.att.com/demos/">AT&amp;T Natural Voices</a> for English, German, Spanish and French.</li>
<li><a  class="external text" title="http://www.cepstral.com/">Cepstral</a> for English, Italian, German, French and Spanish.</li>
<li><a  class="external text" title="http://www.databasesystemscorp.com/psivrtts.htm">DSC Text To Speech Software</a> Applications and demonstration of TTS</li>
<li><a  class="external text" title="http://www.fonixspeech.com">Fonix Speech</a> Offers multiple language TTS engines for embedded and server systems</li>
<li><a  class="external text" title="http://www.ivo.pl/?page=syntezator_mowy_ivona">IVONA TTS</a> for Polish.</li>
<li><a  class="external text" title="http://www.loquendo.com/en/technology/TTS.htm">Loquendo TTS</a> for Dutch, English (UK/US), Spanish, Catalan, Portuguese, Italian, French, German, Greek, Swedish and Chinese. 18 languages and 38 voices. <a  class="external text" title="http://www.loquendo.com/en/demos/demo_tts.htm">Interactive web-based demonstrations available</a>.</li>
<li><a  class="external text" title="http://www.magnetictime.com">MagneticTime</a> allows you to listen to your emails and documents in MP3 on your iPod, mp3 player, PDA or phone.</li>
<li><a  class="external text" title="http://research.microsoft.com/speech/tts">Microsoft Mandarin Chinese TTS Online Demo</a>, <a  class="external text" title="http://research.microsoft.com/speech/engtts">English Demo</a></li>
<li><a  class="external text" title="http://www.meridian-one.co.uk">Orpheus TTS</a> a combined unit-selection and formant synthesis multilingual tts engine.</li>
<li><a  class="external text" title="http://www.nuance.com/realspeak/">RealSpeak</a> by <a  class="external text" title="http://www.nuance.com">Nuance</a> (formerly Scansoft) for English, German, Greek, Spanish and many more.</li>
<li><a  class="external text" title="http://www.research.ibm.com/tts/">IBM Research TTS (U.S. English, Arabic, Chinese, French, &amp; German speech samples)</a></li>
<li><a  class="external text" title="http://www.neospeech.com/product/technologies/tts.php">NeoSpeech VoiceText</a></li>
<li><a  class="external text" title="http://www.sakrament-speech.com">Sakrament Text-to-Speech Engine</a> for Russian and some other languages.</li>
<li><a  class="external text" title="http://www.text2speech.com/">SoftVoice, Inc.</a> Software house that supplied Commodore/Amiga with the speech synthesis in the Amiga OS.</li>
<li><a  class="external text" title="http://www.speechchips.com">speechchips.com</a> source of speech synthesizer chips, chipsets, and development boards.</li>
<li><a  class="external text" title="http://www.speechtech.cz/index-en.php">SpeechTech's ERIS TTS Engine</a> for Czech, Slovak, and German.</li>
<li><a  class="external text" title="http://www.svox.com/">SVOX</a> Swiss specialist for embedded speech output solutions with a portfolio of 18 languages covering all major global markets.</li>
<li><a  class="external text" title="http://www.verbio.com">Verbio TTS - Applied Technologies on Language and Speech (ATLAS)</a> for Spanish.</li>
<li><a  class="external text" title="http://www.vocaloid.com/">Vocaloid</a> Singing TTS by Yamaha</li>
<li><a  class="external text" title="http://www.voicent.com/">Voicent Natural TTS for Telephone</a> is specially designed for playing TTS voice over the phone</li>
<li><a  class="external text" title="http://www.iflytek.com/english/index.htm">iFlyTek InterPhonic</a> is a corpus-based speech synthesis system developed in Chinese venture company.</li>
<li><a  class="external text" title="http://www.voiceware.co.kr/english/demo/demo_text.html">VoiceText</a> is a concatenative speech synthesis system by <a  class="external text" title="http://www.voiceware.co.kr/">kr/english/company/overview.html Voiceware</a>, Korea.</li>
</ul>

<!-- 
Pre-expand include size: 3171 bytes
Post-expand include size: 1466 bytes
Template argument size: 194 bytes
Maximum: 2048000 bytes
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:42799-0!1!0!default!!en!2 and timestamp 20060910144901 -->
<div class="printfooter">
Retrieved from "<a </div>
			<div id="catlinks"><p class='catlinks'><a  title="Special:Categories">Categories</a>: <span dir='ltr'><a  title="Category:Articles with unsourced statements">Articles with unsourced statements</a></span> | <span dir='ltr'><a  title="Category:Artificial intelligence applications">Artificial intelligence applications</a></span> | <span dir='ltr'><a  title="Category:Computational linguistics">Computational linguistics</a></span> | <span dir='ltr'><a  title="Category:Speech synthesis">Speech synthesis</a></span> | <span dir='ltr'><a  class="new" title="Category:Speech processing">Speech processing</a></span> | <span dir='ltr'><a  title="Category:Robotics">Robotics</a></span></p></div>			<!-- end content -->
			<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<ul>
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Speech_synthesis.html">Article</a></li>
				 <li id="ca-talk"><a >Discussion</a></li>
				 <li id="ca-edit"><a >Edit this page</a></li>
				 <li id="ca-history"><a >History</a></li>
		</ul>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a >Sign in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(/images/wiki-en.png);" href="/wiki/Main_Page.html" title="Main Page"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
		<div class='portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage"><a href="/wiki/Main_Page.html">Main Page</a></li>
				<li id="n-portal"><a >Community Portal</a></li>
				<li id="n-Featured-articles"><a >Featured articles</a></li>
				<li id="n-currentevents"><a >Current events</a></li>
				<li id="n-recentchanges"><a >Recent changes</a></li>
				<li id="n-randompage"><a >Random article</a></li>
				<li id="n-help"><a >Help</a></li>
				<li id="n-contact"><a >Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a >Donations</a></li>
			</ul>
		</div>
	</div>
		<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/wiki/Special:Search" id="searchform"><div>
				<input id="searchInput" name="search" type="text" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" value="Search" />
			</div></form>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a >What links here</a></li>
				<li id="t-recentchangeslinked"><a >Related changes</a></li>
<li id="t-upload"><a >Upload file</a></li>
<li id="t-specialpages"><a >Special pages</a></li>
				<li id="t-print"><a >Printable version</a></li>				<li id="t-permalink"><a >Permanent link</a></li><li id="t-cite"><a >Cite this article</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>In other languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-da"><a >Dansk</a></li>
				<li class="interwiki-de"><a >Deutsch</a></li>
				<li class="interwiki-es"><a >Español</a></li>
				<li class="interwiki-eo"><a >Esperanto</a></li>
				<li class="interwiki-eu"><a >Euskara</a></li>
				<li class="interwiki-fa"><a >فارسی</a></li>
				<li class="interwiki-fr"><a >Français</a></li>
				<li class="interwiki-it"><a >Italiano</a></li>
				<li class="interwiki-hu"><a >Magyar</a></li>
				<li class="interwiki-ms"><a >Bahasa Melayu</a></li>
				<li class="interwiki-nl"><a >Nederlands</a></li>
				<li class="interwiki-ja"><a >日本語</a></li>
				<li class="interwiki-nn"><a >Norsk (nynorsk)</a></li>
				<li class="interwiki-pl"><a >Polski</a></li>
				<li class="interwiki-ru"><a >Русский</a></li>
				<li class="interwiki-fi"><a >Suomi</a></li>
				<li class="interwiki-sv"><a >Svenska</a></li>
				<li class="interwiki-zh"><a >中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a ><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="MediaWiki" /></a></div>
				<div id="f-copyrightico"><a ><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
				<li id="lastmod"> This page was last modified 03:01, 10 September 2006.</li>
				<li id="copyright">All text is available under the terms of the <a class='internal'  title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal'  title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the Wikimedia Foundation, Inc.<br /></li>
				<li id="privacy"><a  title="wikimedia:Privacy policy">Privacy policy</a></li>
				<li id="about"><a  title="Wikipedia:About">About Wikipedia</a></li>
				<li id="disclaimer"><a  title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
		
	
		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
</div>
<!-- Served by srv117 in 0.084 secs. --></body></html>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<meta name="keywords" content="Neural network,1950s,20q,Acetylcholine,Adaptive system,Algorithm,Artificial intelligence,Artificial neural network,Artificial neuron,Autonomous robot,Axons" />
<link rel="shortcut icon"  />
<link rel="search" type="application/opensearchdescription+xml"  />
<link rel="copyright"  />
		<title>Neural network - Wikipedia, the free encyclopedia</title>
		<style type="text/css" media="screen,projection">/*<![CDATA[*/ @import "/skins-1.5/monobook/main.css?9"; /*]]>*/</style>
		<link rel="stylesheet" type="text/css" media="print"  />
		<!--[if lt IE 5.5000]><style type="text/css">@import "/skins-1.5/monobook/IE50Fixes.css";</style><![endif]-->
		<!--[if IE 5.5000]><style type="text/css">@import "/skins-1.5/monobook/IE55Fixes.css";</style><![endif]-->
		<!--[if IE 6]><style type="text/css">@import "/skins-1.5/monobook/IE60Fixes.css";</style><![endif]-->
		<!--[if IE 7]><style type="text/css">@import "/skins-1.5/monobook/IE70Fixes.css?1";</style><![endif]-->
		<!--[if lt IE 7]><script type="text/javascript" src="/skins-1.5/common/IEFixes.js"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->
		
		<script type= "text/javascript">
			var skin = "monobook";
			var stylepath = "/skins-1.5";

			var wgArticlePath = "/wiki/$1";
			var wgScriptPath = "/w";
			var wgServer = "http://en.wikipedia.org";
                        
			var wgCanonicalNamespace = "";
			var wgNamespaceNumber = 0;
			var wgPageName = "Neural_network";
			var wgTitle = "Neural network";
			var wgArticleId = 1729542;
			var wgIsArticle = true;
                        
			var wgUserName = null;
			var wgUserLanguage = "en";
			var wgContentLanguage = "en";
		</script>
		                
		<script type="text/javascript" src="/skins-1.5/common/wikibits.js?1"><!-- wikibits js --></script>
		<script type="text/javascript" src="/w/index.php?title=-&amp;action=raw&amp;gen=js"><!-- site js --></script>
		<style type="text/css">/*<![CDATA[*/
@import "/w/index.php?title=MediaWiki:Common.css&usemsgcache=yes&action=raw&ctype=text/css&smaxage=2678400";
@import "/w/index.php?title=MediaWiki:Monobook.css&usemsgcache=yes&action=raw&ctype=text/css&smaxage=2678400";
@import "/w/index.php?title=-&action=raw&gen=css&maxage=2678400";
/*]]>*/</style>
		<!-- Head Scripts -->
		<script type="text/javascript" src="/skins-1.5/common/ajax.js"></script>
	</head>
<body  class="mediawiki ns-0 ltr">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
		<div id="siteNotice"><div style="text-align:right; font-size:80%">Your <b><a  class="extiw" title="wikimedia:Fundraising">continued donations</a></b> keep Wikipedia running!&nbsp;&nbsp;&nbsp;&nbsp;</div>
</div>		<h1 class="firstHeading">Neural network</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From Wikipedia, the free encyclopedia</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a >navigation</a>, <a >search</a></div>			<!-- start content -->
			<div class="thumb tright">
<div style="width:172px;"><a  class="internal" title="Simplified view of an artificial neural network"><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Neuralnetwork.png/170px-Neuralnetwork.png" alt="Simplified view of an artificial neural network" width="170" height="106" longdesc="/wiki/Image:Neuralnetwork.png" /></a>
<div class="thumbcaption">
<div class="magnify" style="float:right"><a  class="internal" title="Enlarge"><img src="/skins-1.5/common/images/magnify-clip.png" width="15" height="11" alt="Enlarge" /></a></div>
Simplified view of an artificial neural network</div>
</div>
</div>
<p>A <b>neural network</b> is a <a href="/wiki/System.html" title="System">system</a> of interconnecting <a href="/wiki/Neuron.html" title="Neuron">neurons</a> in a <a href="/wiki/Network.html" title="Network">network</a> working together to produce an output function. The output of a neural network relies on the cooperation of the individual neurons within the network to operate. Processing of information by neural networks is often done <a  class="new" title="In parallel">in parallel</a> rather than <a  class="new" title="In series">in series</a> (or sequentially). Since it relies on its member neurons collectively to perform its function, a unique property of a neural network is that it can still perform its overall function even if some of the neurons are not functioning. That is, they are very <a href="/wiki/Robust.html" title="Robust">robust</a> to error or failure (i.e., <a href="/wiki/Fault_tolerant.html" title="Fault tolerant">fault tolerant</a>).</p>
<p><b>Neural network</b> is sometimes used to refer to a branch of <a href="/wiki/Computational_science.html" title="Computational science">computational science</a> that uses neural networks as models to either simulate or analyze complex phenomena and/or studies the principles of operation of neural networks analytically. It addresses problems similar to <a href="/wiki/Artificial_intelligence.html" title="Artificial intelligence">artificial intelligence</a> (AI) except that AI uses traditional computational <a href="/wiki/Algorithm.html" title="Algorithm">algorithms</a> to solve problems whereas neural networks uses neural network as the computational architecture to solve problems. Well-designed neural networks are trainable systems that can often "<a href="/wiki/Learn.html" title="Learn">learn</a>" to solve complex problems from a set of <a  class="new" title="Examplar">examplars</a> and generalize the "acquired knowledge" to solve unforeseen problems, i.e., they are self-<a href="/wiki/Adaptive_system.html" title="Adaptive system">adaptive systems</a>.</p>
<p>Traditionally, a neural network is used to refer to a network of <a href="/wiki/Neuron.html" title="Neuron">biological neurons</a>. In modern usage, the term is often used to refer to <a href="/wiki/Artificial_neural_network.html" title="Artificial neural network">artificial neural networks</a>, which are composed of <a href="/wiki/Artificial_neuron.html" title="Artificial neuron">artificial neurons</a>. Thus the term 'Neural Network' has two distinct connotations:</p>
<ol>
<li><a href="/wiki/Biological_neural_network.html" title="Biological neural network">Biological neural networks</a> are made up of real biological neurons that are connected or functionally-related in the <a href="/wiki/Peripheral_nervous_system.html" title="Peripheral nervous system">peripheral nervous system</a> or the <a href="/wiki/Central_nervous_system.html" title="Central nervous system">central nervous system</a>. In the field of <a href="/wiki/Neuroscience.html" title="Neuroscience">neuroscience</a>, they are often identified as groups of neurons that perform a specific physiological function in laboratory analysis.</li>
<li><a href="/wiki/Artificial_neural_network.html" title="Artificial neural network">Artificial neural networks</a> are made up of interconnecting artificial neurons (usually simplified neurons) designed to model (or mimick) some properties of biological neural networks. Artificial neural networks can be used to model the modes of operation of biological neural networks, whereas <a href="/wiki/Cognitive_model.html" title="Cognitive model">cognitive models</a> are theoretical models that mimick cognitive brain functions without necessarily using neural networks while artificial intelligence are well-crafted algorithms that solve specific intelligent problems (such as chess playing, pattern recognition, etc.) without using neural network as the computational architecture.</li>
</ol>
<p>Please see the corresponding articles for details on artificial neural networks or biological neural networks. This article focuses on the relationship between the two concepts.</p>
<p>Artificial neural networks are made of simplified neurons called "units" that are often assumed to be simple in the sense that their <a href="/wiki/State_%28physics%29.html" title="State (physics)">state</a> can be described by single numbers called "activation" values. Each unit generates an output signal based on its activation. Units are connected to each other very specifically, each connection having an individual "weight" (again described by a single number). Each unit sends its output value to all other units to which they have an outgoing connection. Through these connections, the output of one unit can influence the activations of other units. The unit receiving the connections calculates its activation by taking a <a href="/wiki/Weighted_sum.html" title="Weighted sum">weighted sum</a> of the input signals (i.e. it multiplies each input signal with the weight that corresponds to that connection and adds these products). The output is determined by the activation function based on this activation (e.g. the unit generates output or "fires" if the activation is above a threshold value). Networks learn by changing the weights of the connections.</p>
<table id="toc" class="toc" summary="Contents">
<tr>
<td>
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1"><a ><span class="tocnumber">1</span> <span class="toctext">Characterization</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">2</span> <span class="toctext">The brain, neural networks and computers</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">3</span> <span class="toctext">Neural networks and Artificial intelligence</span></a>
<ul>
<li class="toclevel-2"><a ><span class="tocnumber">3.1</span> <span class="toctext">Background</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">3.2</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-3"><a ><span class="tocnumber">3.2.1</span> <span class="toctext">Real life applications</span></a></li>
</ul>
</li>
<li class="toclevel-2"><a ><span class="tocnumber">3.3</span> <span class="toctext">Neural network software</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">3.4</span> <span class="toctext">Learning paradigms</span></a>
<ul>
<li class="toclevel-3"><a ><span class="tocnumber">3.4.1</span> <span class="toctext">Supervised learning</span></a></li>
<li class="toclevel-3"><a ><span class="tocnumber">3.4.2</span> <span class="toctext">Unsupervised learning</span></a></li>
<li class="toclevel-3"><a ><span class="tocnumber">3.4.3</span> <span class="toctext">Reinforcement learning</span></a></li>
<li class="toclevel-3"><a ><span class="tocnumber">3.4.4</span> <span class="toctext">Learning algorithms</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1"><a ><span class="tocnumber">4</span> <span class="toctext">Neural networks and neuroscience</span></a>
<ul>
<li class="toclevel-2"><a ><span class="tocnumber">4.1</span> <span class="toctext">Types of models</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">4.2</span> <span class="toctext">Current research</span></a></li>
<li class="toclevel-2"><a ><span class="tocnumber">4.3</span> <span class="toctext">References</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a ><span class="tocnumber">5</span> <span class="toctext">History of the neural network analogy</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">6</span> <span class="toctext">Criticism</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1"><a ><span class="tocnumber">9</span> <span class="toctext">External links</span></a></li>
</ul>
</td>
</tr>
</table>
<p><script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script></p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Characterization">edit</a>]</div>
<p><a name="Characterization" id="Characterization"></a></p>
<h2>Characterization</h2>
<p>In general, a biological neural network is composed of a group or groups of physically connected or functionally associated neurons. A single neuron can be connected to many other neurons and the total number of neurons and connections in a network can be extremely large. Connections, called <a href="/wiki/Synapses.html" title="Synapses">synapses</a>, are usually formed from <a href="/wiki/Axons.html" title="Axons">axons</a> to <a href="/wiki/Dendrites.html" title="Dendrites">dendrites</a>, though dendrodentritic microcircuits [Arbib, p.666] and other connections are possible. Apart from the electrical signalling, there are other forms of signaling that arise from <a href="/wiki/Neurotransmitter.html" title="Neurotransmitter">neurotransmitter</a> diffusion, which have an effect on electrical signaling. As such, neural networks are extremely complex. While a detailed description of neural systems seems currently unattainable, progress is made towards a better understanding of basic mechanisms.</p>
<p><a href="/wiki/Artificial_intelligence.html" title="Artificial intelligence">Artificial intelligence</a> and <a href="/wiki/Cognitive_modeling.html" title="Cognitive modeling">cognitive modeling</a> try to simulate some properties of neural networks. While similar in their techniques, the former has the aim of solving particular tasks, while the latter aims to build mathematical models of biological neural systems.</p>
<p>In the <a href="/wiki/Artificial_intelligence.html" title="Artificial intelligence">artificial intelligence</a> field, artificial neural networks have been applied successfully to <a href="/wiki/Speech_recognition.html" title="Speech recognition">speech recognition</a>, <a href="/wiki/Image_analysis.html" title="Image analysis">image analysis</a> and adaptive <a href="/wiki/Control.html" title="Control">control</a>, in order to construct <a href="/wiki/Software_agents.html" title="Software agents">software agents</a> (in <a href="/wiki/Computer_and_video_games.html" title="Computer and video games">computer and video games</a>) or <a href="/wiki/Autonomous_robot.html" title="Autonomous robot">autonomous robots</a>. Most of the currently employed artificial neural networks for artificial intelligence are based on <a href="/wiki/Statistical_estimation.html" title="Statistical estimation">statistical estimation</a>, <a href="/wiki/Optimisation.html" title="Optimisation">optimisation</a> and <a href="/wiki/Control_theory.html" title="Control theory">control theory</a>.</p>
<p>The <a href="/wiki/Cognitive_modelling.html" title="Cognitive modelling">cognitive modelling</a> field is the physical or mathematical modelling of the behaviour of neural systems; ranging from the individual neural level (e.g. modelling the spike response curves of neurons to a stimulus), through the neural cluster level (e.g. modelling the release and effects of dopamine in the basal ganglia) to the complete organism (e.g. behavioural modelling of the organism's response to stimuli).</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: The brain, neural networks and computers">edit</a>]</div>
<p><a name="The_brain.2C_neural_networks_and_computers" id="The_brain.2C_neural_networks_and_computers"></a></p>
<h2>The brain, neural networks and computers</h2>
<p>While historically the brain has been viewed as a type of computer, and vice-versa, this is true only in the loosest sense. Computers do not provide us with accurate hardware for describing the brain (even though it is possible to describe a logical process as a computer program, or to simulate a brain using a computer) as they do not posses the parallel processing architectures that have been described in the brain. Even when speaking of multiprocessor computers, the functions are not nearly as distributed as in the brain.</p>
<p>Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and brain biological architecture is very much debated. To answer this question, Marr has proposed various levels of analysis which provide us with a plausible answer for the role of neural networks in the understanding of human cognitive functioning.</p>
<p>The question of what is the degree of complexity and the properties that individual neural elements should have in order to reproduce something resembling animal intelligence is a subject of current research in theoretical neuroscience.</p>
<p>Historically computers evolved from Von Neumann architecture, based on sequential processing and execution of explicit instructions. On the other hand origins of neural networks are based on efforts to model information processing in biological systems, which are primarily based on parallel processing as well as implicit instructions based on recognition of patterns of 'sensory' input from external sources. In other words, rather sequential processing and execution, at their very heart, neural networks are complex statistic processors.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Neural networks and Artificial intelligence">edit</a>]</div>
<p><a name="Neural_networks_and_Artificial_intelligence" id="Neural_networks_and_Artificial_intelligence"></a></p>
<h2>Neural networks and Artificial intelligence</h2>
<p><i>Main article: <a href="/wiki/Artificial_neural_network.html" title="Artificial neural network">Artificial neural network</a></i></p>
<p>An <b>artificial neural network</b> (ANN), also called a <b>simulated neural network</b> (SNN) or commonly just <b>neural network</b> (NN) is an interconnected group of <a href="/wiki/Artificial_neuron.html" title="Artificial neuron">artificial neurons</a> that uses a <a href="/wiki/Mathematical_model.html" title="Mathematical model">mathematical or computational model</a> for <a href="/wiki/Information_processing.html" title="Information processing">information processing</a> based on a <a href="/wiki/Connectionism.html" title="Connectionism">connectionist</a> approach to <a href="/wiki/Computation.html" title="Computation">computation</a>. In most cases an ANN is an <a href="/wiki/Adaptive_system.html" title="Adaptive system">adaptive system</a> that changes its structure based on external or internal information that flows through the network.</p>
<p>In more practical terms neural networks are <a href="/wiki/Non-linear.html" title="Non-linear">non-linear</a> <a href="/wiki/Statistical.html" title="Statistical">statistical</a> <a href="/wiki/Data_modeling.html" title="Data modeling">data modeling</a> tools. They can be used to model complex relationships between inputs and outputs or to <a href="/wiki/Pattern_recognition.html" title="Pattern recognition">find patterns</a> in data.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Background">edit</a>]</div>
<p><a name="Background" id="Background"></a></p>
<h3>Background</h3>
<p>An <a href="/wiki/Artificial_neural_network.html" title="Artificial neural network">artificial neural network</a> involves a network of simple processing elements (<a href="/wiki/Neurons.html" title="Neurons">neurons</a>) which can exhibit complex global behaviour, determined by the connections between the processing elements and element parameters.</p>
<p>In a neural network model, simple <a href="/wiki/Node_%28neural_networks%29.html" title="Node (neural networks)">nodes</a> (called variously "neurons", "neurodes", "PEs" ("processing elements") or "units") are connected together to form a network of nodes — hence the term "neural network". While a neural network does not have to be adaptive per se, its practical use comes with algorithms designed to alter the strength (weights) of the connections in the network to produce a desired signal flow.</p>
<p>In modern <a href="/wiki/Neural_network_software.html" title="Neural network software">software implementations</a> of artificial neural networks the approach inspired by biology has more or less been abandoned for a more practical approach based on statistics and signal processing. In some of these systems neural networks, or parts of neural networks (such as <a href="/wiki/Artificial_neuron.html" title="Artificial neuron">artificial neurons</a>) are used as components in larger systems that combine both adaptive and non-adaptive elements.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Applications">edit</a>]</div>
<p><a name="Applications" id="Applications"></a></p>
<h3>Applications</h3>
<p>The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations. This is particularly useful in applications where the complexity of the data or task makes the design of such a function by hand impractical.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Real life applications">edit</a>]</div>
<p><a name="Real_life_applications" id="Real_life_applications"></a></p>
<h4>Real life applications</h4>
<p>The tasks to which artificial neural networks are applied tend to fall within the following broad categories:</p>
<ul>
<li><a href="/wiki/Function_approximation.html" title="Function approximation">Function approximation</a>, or <a href="/wiki/Regression_analysis.html" title="Regression analysis">regression analysis</a>, including <a href="/wiki/Time_series_prediction.html" title="Time series prediction">time series prediction</a> and modelling.</li>
<li><a href="/wiki/Statistical_classification.html" title="Statistical classification">Classification</a>, including <a href="/wiki/Pattern_recognition.html" title="Pattern recognition">pattern</a> and sequence recognition, novelty detection and sequential decision making.</li>
<li><a href="/wiki/Data_processing.html" title="Data processing">Data processing</a>, including filtering, clustering, <a href="/wiki/Blind_source_separation.html" title="Blind source separation">blind source separation</a> and compression.</li>
</ul>
<p>Application areas include system identification and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition and more), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, <a href="/wiki/Data_mining.html" title="Data mining">data mining</a> (or knowledge discovery in databases, "KDD"), visualisation and <a href="/wiki/E-mail_spam.html" title="E-mail spam">e-mail spam</a> filtering.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Neural network software">edit</a>]</div>
<p><a name="Neural_network_software" id="Neural_network_software"></a></p>
<h3>Neural network software</h3>
<p><i>Main article:</i> <a href="/wiki/Neural_network_software.html" title="Neural network software">Neural network software</a></p>
<p><b>Neural network software</b> is used to <a href="/wiki/Simulation.html" title="Simulation">simulate</a>, <a href="/wiki/Research.html" title="Research">research</a>, <a href="/wiki/Development.html" title="Development">develop</a> and apply <a href="/wiki/Artificial_neural_network.html" title="Artificial neural network">artificial neural networks</a>, <a href="/wiki/Biological_neural_network.html" title="Biological neural network">biological neural networks</a> and in some cases a wider array of <a href="/wiki/Adaptive_system.html" title="Adaptive system">adaptive systems</a>.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Learning paradigms">edit</a>]</div>
<p><a name="Learning_paradigms" id="Learning_paradigms"></a></p>
<h3>Learning paradigms</h3>
<p>There are three major learning paradigms, each corresponding to a particular abstract learning task. These are <a href="/wiki/Supervised_learning.html" title="Supervised learning">supervised learning</a>, <a href="/wiki/Unsupervised_learning.html" title="Unsupervised learning">unsupervised learning</a> and <a href="/wiki/Reinforcement_learning.html" title="Reinforcement learning">reinforcement learning</a>. Usually any given type of network architecture can be employed in any of those tasks.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Supervised learning">edit</a>]</div>
<p><a name="Supervised_learning" id="Supervised_learning"></a></p>
<h4>Supervised learning</h4>
<p>In <a href="/wiki/Supervised_learning.html" title="Supervised learning">supervised learning</a>, we are given a set of example pairs <img class='tex' src="http://upload.wikimedia.org/math/1/e/3/1e39a260d546fe7c3cd1f9a42979dc71.png" alt="(x, y), x \in X, y \in Y" /> and the aim is to find a function f in the allowed class of functions that matches the examples. In other words, we wish to <i>infer</i> the mapping implied by the data and the cost function is related to the mismatch between our mapping and the data.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Unsupervised learning">edit</a>]</div>
<p><a name="Unsupervised_learning" id="Unsupervised_learning"></a></p>
<h4>Unsupervised learning</h4>
<p>In <a href="/wiki/Unsupervised_learning.html" title="Unsupervised learning">unsupervised learning</a> we are given some data <span class="texhtml"><i>x</i></span>, and the cost function to be minimised can be any function of the data <span class="texhtml"><i>x</i></span> and the network's output, <span class="texhtml"><i>f</i></span>. The cost function is determined by the task formulation. Most applications fall within the domain of <a  class="new" title="Estimation problems">estimation problems</a> such as <a href="/wiki/Statistical_modelling.html" title="Statistical modelling">statistical modelling</a>, <a href="/wiki/Data_compression.html" title="Data compression">compression</a>, <a href="/wiki/Mail_filter.html" title="Mail filter">filtering</a>, <a href="/wiki/Blind_source_separation.html" title="Blind source separation">blind source separation</a> and <a href="/wiki/Data_clustering.html" title="Data clustering">clustering</a>.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Reinforcement learning">edit</a>]</div>
<p><a name="Reinforcement_learning" id="Reinforcement_learning"></a></p>
<h4>Reinforcement learning</h4>
<p>In <a href="/wiki/Reinforcement_learning.html" title="Reinforcement learning">reinforcement learning</a>, data <span class="texhtml"><i>x</i></span> is usually not given, but generated by an agent's interactions with the environment. At each point in time <span class="texhtml"><i>t</i></span>, the agent performs an action <span class="texhtml"><i>y</i><sub><i>t</i></sub></span> and the environment generates an observation <span class="texhtml"><i>x</i><sub><i>t</i></sub></span> and an instantaneous cost <span class="texhtml"><i>c</i><sub><i>t</i></sub></span>, according to some (usually unknown) dynamics. The aim is to discover a <i>policy</i> for selecting actions that minimises some measure of a long-term cost, i.e. the expected cumulative cost. The environment's dynamics and the long-term cost for each policy are usually unknown, but can be estimated. ANNs are frequently used in reinforcement learning as part of the overall algorithm. Tasks that fall within the paradigm of reinforcement learning are <a href="/wiki/Control.html" title="Control">control</a> problems, <a href="/wiki/Games.html" title="Games">games</a> and other <a  class="new" title="Sequential decision making">sequential decision making</a> tasks.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Learning algorithms">edit</a>]</div>
<p><a name="Learning_algorithms" id="Learning_algorithms"></a></p>
<h4>Learning algorithms</h4>
<p>There are numerous algorithms available for training neural network models; most of them can be viewed as a straightforward application of <a href="/wiki/Optimization_%28mathematics%29.html" title="Optimization (mathematics)">optimization</a> theory and <a href="/wiki/Statistical_estimation.html" title="Statistical estimation">statistical estimation</a>.</p>
<p>Most of the algorithms used in training artificial neural networks are employing some form of <a href="/wiki/Gradient_descent.html" title="Gradient descent">gradient descent</a>. This is done by simply taking the derivative of the cost function with respect to the network parameters and then changing those parameters in a <a href="/wiki/Gradient-related.html" title="Gradient-related">gradient-related</a> direction.</p>
<p><a href="/wiki/Evolutionary_computation.html" title="Evolutionary computation">Evolutionary computation</a> methods, <a href="/wiki/Simulated_annealing.html" title="Simulated annealing">simulated annealing</a>, <a href="/wiki/Expectation-Maximization.html" title="Expectation-Maximization">expectation maximization</a> and <a href="/wiki/Non-parametric_methods.html" title="Non-parametric methods">non-parametric methods</a> are among other commonly used methods for training neural networks. See also <a href="/wiki/Machine_learning.html" title="Machine learning">machine learning</a>.</p>
<p>Recent developments in this field also saw the use of <a href="/wiki/Particle_swarm_optimization.html" title="Particle swarm optimization">Particle Swarm Optimizations</a> and other <a href="/wiki/Swarm_Intelligence.html" title="Swarm Intelligence">Swarm Intelligence</a> techniques used in the training of neural networks, in this case fitness would be defined as the accuracy of the neural network.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Neural networks and neuroscience">edit</a>]</div>
<p><a name="Neural_networks_and_neuroscience" id="Neural_networks_and_neuroscience"></a></p>
<h2>Neural networks and neuroscience</h2>
<p>Theoretical and computational neuroscience is the field concerned with the theoretical analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.</p>
<p>The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Types of models">edit</a>]</div>
<p><a name="Types_of_models" id="Types_of_models"></a></p>
<h3>Types of models</h3>
<p>Many models are used in the field, each defined at a different level of abstraction and trying to model different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of how the dynamics of neural circuitry arise from interactions between individual neurons, to models of how behaviour can arise from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and its relation to learning and memory, from the individual neuron to the system level.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Current research">edit</a>]</div>
<p><a name="Current_research" id="Current_research"></a></p>
<h3>Current research</h3>
<p>While initially research had been concerned mostly with the electrical characteristics of neurons, a particularly important part of the investigation in recent years has been the exploration of the role of <a href="/wiki/Neuromodulators.html" title="Neuromodulators">neuromodulators</a> such as <a href="/wiki/Dopamine.html" title="Dopamine">dopamine</a>, <a href="/wiki/Acetylcholine.html" title="Acetylcholine">acetylcholine</a>, and <a href="/wiki/Serotonin.html" title="Serotonin">serotonin</a> on behaviour and learning.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: References">edit</a>]</div>
<p><a name="References" id="References"></a></p>
<h3>References</h3>
<ul>
<li><cite class="book" style="font-style:normal">Peter Dayan, L.F. Abbott. <i>Theoretical Neuroscience</i>. MIT Press.</cite></li>
<li><cite class="book" style="font-style:normal">Wulfram Gerstner, Werner Kistler. <i>Spiking Neuron Models:Single Neurons, Populations, Plasticity</i>. Cambridge University Press.</cite></li>
<li>Van den Bergh, F. Engelbrecht, AP. "<i>Cooperative Learning in Neural Networks using Particle Swarm Optimizers</i>". CIRG 2000.</li>
</ul>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: History of the neural network analogy">edit</a>]</div>
<p><a name="History_of_the_neural_network_analogy" id="History_of_the_neural_network_analogy"></a></p>
<h2>History of the neural network analogy</h2>
<p><i>(main article: <a href="/wiki/Connectionism.html" title="Connectionism">Connectionism</a>)</i></p>
<p>The concept of neural networks started in the late-1800s as an effort to describe how the human mind performed. These ideas started being applied to computational models with the <a href="/wiki/Perceptron.html" title="Perceptron">Perceptron</a>.</p>
<p>In early <a href="/wiki/1950s.html" title="1950s">1950s</a> <a href="/wiki/Friedrich_Hayek.html" title="Friedrich Hayek">Friedrich Hayek</a> was one of the first to posit the idea of spontaneous order in the brain arising out of decentralized networks of simple units (neurons). In the late 1940s, <a href="/wiki/Donald_Hebb.html" title="Donald Hebb">Donald Hebb</a> made one of the first hypotheses for a mechanism of neural plasticity (i.e. learning), <a href="/wiki/Hebbian_learning.html" title="Hebbian learning">Hebbian learning</a>. Hebbian learning is considered be a 'typical' unsupervised learning rule and it (and variants of it) was an early model for <a href="/wiki/Long_term_potentiation.html" title="Long term potentiation">long term potentiation</a>.</p>
<p>The <a href="/wiki/Perceptron.html" title="Perceptron">Perceptron</a> is essentially a linear classifier for classifying data <img class='tex' src="http://upload.wikimedia.org/math/1/5/1/151a50d8a4d603d32da96befeb8fce48.png" alt="x \in R^n" /> specified by parameters <img class='tex' src="http://upload.wikimedia.org/math/0/4/8/04879c895a3a4b40810e442c23b4e61b.png" alt="w \in R^n, b \in R" /> and an output function <span class="texhtml"><i>f</i> = <i>w</i>'<i>x</i> + <i>b</i></span>. Its parameters are adapted with an ad-hoc rule similar to stochastic steepest gradient descent. Because the inner product is linear operator in the input space, the Perceptron can only perfectly classify a set of data for which different classes are <a href="/wiki/Linearly_separable.html" title="Linearly separable">linearly separable</a> in the input space, while it often fails completely for non-separable data. While the development of the algorithm initially generated some enthusiasm, partly because of its apparent relation to biological mechanisms, the later discovery of this inadequacy caused such models to be abandoned until the introduction of non-linear models into the field.</p>
<p>The <a  class="new" title="Cognitron">Cognitron</a> (1975) was an early multilayered neural network with a training algorithm. The actual structure of the network and the methods used to set the interconnection weights change from one neural strategy to another, each with its advantages and disadvantages. Networks can propagate information in one direction only, or they can bounce back and forth until self-activation at a node occurs and the network settles on a final state. The ability for bi-directional flow of inputs between neurons/nodes was produced with the <a href="/wiki/Hopfield_net.html" title="Hopfield net">Hopfield's network</a> (1982), and specialization of these node layers for specific purposes was introduced through the first <a href="/wiki/Hybrid_neural_network.html" title="Hybrid neural network">hybrid network</a>.</p>
<p>The <a href="/wiki/Connectionism.html" title="Connectionism">parallel distributed processing</a> of the mid-1980s became popular under the name <a href="/wiki/Connectionism.html" title="Connectionism">connectionism</a>.</p>
<p>The <a href="/wiki/Backpropagation.html" title="Backpropagation">backpropagation</a> network was probably the main reason behind the repopularisation of neural networks after the publication of "Learning Internal Representations by Error Propagation" in 1986. The original network utilised multiple layers of weight-sum units of the type <span class="texhtml"><i>f</i> = <i>g</i>(<i>w</i>'<i>x</i> + <i>b</i>)</span>, where <span class="texhtml"><i>g</i></span> was a <a href="/wiki/Sigmoid_function.html" title="Sigmoid function">sigmoid function</a>. Training was done by a form of stochastic steepest gradient descent. The employment of the chain rule of differentiation in deriving the appropriate parameter updates results in an algorithm that seems to 'backpropagate errors', hence the nomenclature. However it is essentially a form of gradient descent. Determining the optimal parameters in a model of this type is not trivial, and steepest gradient descent methods cannot be relied upon to give the solution without a good starting point. In recent times, networks with the same architecture as the backpropagation network are referred to as Multi-Layer Perceptrons. This name does not impose any limitations on the type of algorithm used for learning.</p>
<p>The backpropagation network generated much enthusiasm at the time and there was much controversy about whether such learning could be implemented in the brain or not, partly because a mechanism for reverse signalling was not obvious at the time, but most importantly because there was no plausible source for the 'teaching' or 'target' signal.</p>
<p>In more recent times, neuroscientists have successfully made some associations between reinforcement learning and the dopamine system of reward. However, the role of this and other <a href="/wiki/Neuromodulators.html" title="Neuromodulators">neuromodulators</a> is still under active investigation.</p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: Criticism">edit</a>]</div>
<p><a name="Criticism" id="Criticism"></a></p>
<h2>Criticism</h2>
<p>A. K. Dewdney, a former Scientific American columnist, wrote in 1997, <i>“Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool.”</i> (Dewdney, p.82)</p>
<p>Arguments against Dewdney's position are that neural nets have been successfully used to solve many complex and diverse tasks, ranging from autonomously flying aircrafts <a  class="external autonumber" title="http://www.nasa.gov/centers/dryden/news/NewsReleases/2003/03-49.html">[1]</a> to detecting credit card fraud <a  class="external autonumber" title="http://www.visa.ca/en/personal/shop_neural.cfm">[2]</a>.</p>
<p>Technology writer Roger Bridgman commented on Dewdney's statements about neural nets: <i>"Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be "an opaque, unreadable table...valueless as a scientific resource".</i></p>
<p><i>In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having."</i><a  class="external autonumber" title="http://members.fortunecity.com/templarseries/popper.html">[3]</a></p>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: See also">edit</a>]</div>
<p><a name="See_also" id="See_also"></a></p>
<h2>See also</h2>
<ul>
<li><a href="/wiki/Artificial_neural_network.html" title="Artificial neural network">Artificial neural network</a></li>
<li><a href="/wiki/Cognitive_architecture.html" title="Cognitive architecture">Cognitive architecture</a></li>
<li><a href="/wiki/Biologically-inspired_computing.html" title="Biologically-inspired computing">Biologically-inspired computing</a></li>
<li><a href="/wiki/Parallel_distributed_processing.html" title="Parallel distributed processing">Parallel distributed processing</a></li>
<li><a href="/wiki/Radial_basis_function.html" title="Radial basis function">Radial basis function</a></li>
<li><a href="/wiki/Biological_cybernetics.html" title="Biological cybernetics">Biological cybernetics</a></li>
<li><a href="/wiki/Distributed_representation.html" title="Distributed representation">Distributed representation</a></li>
<li><a href="/wiki/Tensor_product_network.html" title="Tensor product network">Tensor product network</a></li>
<li><a href="/wiki/Neuro-fuzzy.html" title="Neuro-fuzzy">Neuro-fuzzy</a></li>
<li><a href="/wiki/Neural_network_software.html" title="Neural network software">Neural network software</a></li>
<li><a href="/wiki/Predictive_analytics.html" title="Predictive analytics">Predictive analytics</a></li>
<li><a href="/wiki/20q.html" title="20q">20q</a> is a neural network implementation of the 20 questions game</li>
</ul>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: References">edit</a>]</div>
<p><a name="References_2" id="References_2"></a></p>
<h2>References</h2>
<ul>
<li>Abdi, H. "<i><a  class="external text" title="http://www.utdallas.edu/~herve/abdi.primer.pdf">A neural network primer. Journal of Biological Systems, 2, 247-281, (1994)</a></i>".</li>
<li>Abdi, H. "<i><a  class="external autonumber" title="http://www.utdallas.edu/~herve/Abdi-Neural-networks-pretty.pdf">[4]</a> (2003). Neural Networks. In M. Lewis-Beck, A. Bryman, T. Futing (Eds): Encyclopedia for research methods for the social sciences. Thousand Oaks (CA): Sage. pp. 792-795.]</i>".</li>
<li>Abdi, H. "<i><a  class="external autonumber" title="http://www.utdallas.edu/~herve/Abdi-lann-01.pdf">[5]</a> (2001). Linear algebra for neural networks. In N.J. Smelser, P.B. Baltes (Eds.): International Encyclopedia of the Social and Behavioral Sciences. Oxford (UK): Elsevier.]</i>".</li>
<li>Abdi, H., Valentin, D., Edelman, B.E. (1999). <i>Neural Networks.</i> Thousand Oaks: Sage.</li>
<li><cite class="book" style="font-style:normal">Anderson, James A. (1995). <i>An Introduction to Neural Networks</i>. <a  class="internal">ISBN 0-262-01144-1</a>.</cite></li>
<li><cite class="book" style="font-style:normal">Arbib, Michael A. (Ed.) (1995). <i>The Handbook of Brain Theory and Neural Networks</i>.</cite></li>
<li>Alspector, <a  class="external text" title="http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4874963">U.S. Patent 4874963</a> "<i>Neuromorphic learning networks</i>". October 17, 1989.</li>
<li><cite class="book" style="font-style:normal">Agre, Philip E., et al. (1997). <i>Comparative Cognitive Robotics: Computation and Human Experience</i>. Cambridge University Press. <a  class="internal">ISBN 0-521-38603-9</a>.</cite>, p. 80</li>
<li><cite class="book" style="font-style:normal">Bar-Yam, Yaneer (2003). <i><a  class="external text" title="http://necsi.org/publications/dcs/Bar-YamChap2.pdf">Dynamics of Complex Systems, Chapter 2</a></i>.</cite></li>
<li><cite class="book" style="font-style:normal">Bar-Yam, Yaneer (2003). <i><a  class="external text" title="http://necsi.org/publications/dcs/Bar-YamChap3.pdf">Dynamics of Complex Systems, Chapter 3</a></i>.</cite></li>
<li><cite class="book" style="font-style:normal">Bar-Yam, Yaneer (2005). <i><a  class="external text" title="http://necsi.org/publications/mtw/">Making Things Work</a></i>.</cite> Please see Chapter 3</li>
<li><cite class="book" style="font-style:normal">Bertsekas, Dimitri P. (1999). <i>Nonlinear Programming</i>.</cite></li>
<li><cite class="book" style="font-style:normal">Bertsekas, Dimitri P. &amp; Tsitsiklis, John N. (1996). <i>Neuro-dynamic Programming</i>.</cite></li>
<li><cite class="book" style="font-style:normal">Boyd, Stephen &amp; Vandenberghe, Lieven (2004). <i><a  class="external text" title="http://www.stanford.edu/~boyd/cvxbook/">Convex Optimization</a></i>.</cite></li>
<li><cite class="book" style="font-style:normal">Dewdney, A. K. (1997). <i>Yes, We Have No Neutrons: An Eye-Opening Tour through the Twists and Turns of Bad Science</i>.</cite></li>
<li><cite style="font-style:normal">Fukushima, K. (1975). "Cognitron: A Self-Organizing Multilayered Neural Network". <i>Biological Cybernetics</i> <b>20</b>: 121–136.</cite></li>
<li><cite style="font-style:normal">Frank, Michael J. (2005). "Dynamic Dopamine Modulation in the Basal Ganglia: A Neurocomputational Account of Cognitive Deficits in Medicated and Non-medicated Parkinsonism". <i>Journal of Cognitive Neuroscience</i> <b>17</b>: 51–72.</cite></li>
<li><cite style="font-style:normal">Gardner, E.J., &amp; Derrida, B. (1988). "Optimal storage properties of neural network models". <i>Journal of Physics A</i> <b>21</b>: 271–284.</cite></li>
<li><cite style="font-style:normal">Krauth, W., &amp; Mezard, M. (1989). "Storage capacity of memory with binary couplings". <i>Journal de Physique</i> <b>50</b>: 3057–3066.</cite></li>
<li><cite style="font-style:normal">Maass, W., &amp; Markram, H. (2002). "<a  class="external text" title="http://www.igi.tugraz.at/maass/publications.html">On the computational power of recurrent circuits of spiking neurons</a>". <i>Journal of Computer and System Sciences</i> <b>69(4)</b>: 593–616.</cite></li>
<li><cite class="book" style="font-style:normal">MacKay, David (2003). <i><a  class="external text" title="http://www.inference.phy.cam.ac.uk/mackay/itprnn/book.html">Information Theory, Inference, and Learning Algorithms</a></i>.</cite></li>
<li><cite class="book" style="font-style:normal">Mandic, D. &amp; Chambers, J. (2001). <i>Recurrent Neural Networks for Prediction: Architectures, Learning algorithms and Stability</i>. Wiley.</cite></li>
<li><cite class="book" style="font-style:normal">Minsky, M. &amp; Papert, S. (1969). <i>An Introduction to Computational Geometry</i>. MIT Press.</cite></li>
<li><cite style="font-style:normal">Muller, P. &amp; Insua, D.R. (1995). "Issues in Bayesian Analysis of Neural Network Models". <i>Neural Computation</i> <b>10</b>: 571–592.</cite></li>
<li><cite style="font-style:normal">Reilly, D.L., Cooper, L.N. &amp; Elbaum, C. (1982). "A Neural Model for Category Learning". <i>Biological Cybernetics</i> <b>45</b>: 35–41.</cite></li>
<li><cite class="book" style="font-style:normal">Rosenblatt, F. (1962). <i>Principles of Neurodynamics</i>. Spartan Books.</cite></li>
<li><cite class="book" style="font-style:normal">Sutton, Richard S. &amp; Barto, Andrew G. (1998). <i><a  class="external text" title="http://www.cs.ualberta.ca/~sutton/book/the-book.html">Reinforcement Learning&#160;: An introduction</a></i>.</cite></li>
</ul>
<ul>
<li><cite style="font-style:normal">Wilkes, A.L. &amp; Wade, N.J. (1997). "Bain on Neural Networks". <i>Brain and Cognition</i> <b>33</b>: 295–305.</cite></li>
<li><cite class="book" style="font-style:normal">Wasserman, P.D. (1989). <i>Neural computing theory and practice</i>. Van Nostrand Reinhold.</cite></li>
<li>Jeffrey T. Spooner, Manfredi Maggiore, Raul Ord onez, and Kevin M. Passino, Stable Adaptive Control and Estimation for Nonlinear Systems: Neural and Fuzzy Approximator Techniques, John Wiley and Sons, NY, 2002.</li>
</ul>
<div class="editsection" style="float:right;margin-left:5px;">[<a  title="Edit section: External links">edit</a>]</div>
<p><a name="External_links" id="External_links"></a></p>
<h2>External links</h2>
<ul>
<li><a  class="external text" title="http://www.neuralnetworksolutions.com/resources.php">Introduction to Neural Networks</a></li>
<li><a  class="external text" title="http://www.tandf.co.uk/journals/titles/0954898X.asp">Network: Computation in Neural Systems</a></li>
<li><a  class="external text" title="http://www.faqs.org/faqs/ai-faq/neural-nets/">Usenet FAQ: comp.ai.neural-nets</a></li>
<li><a  class="external text" title="http://artsci.wustl.edu/~philos/MindDict/connectionism.html">Connectionism at MindDict</a></li>
<li><a  class="external text" title="http://www.peltarion.com/applications/applications.html">Applications of Neural Networks</a></li>
<li><a  class="external text" title="http://www.willamette.edu/~gorr/classes/cs449/intro.html">Introduction to Artificial Neural Networks</a></li>
<li><a  class="external text" title="http://topographica.org">Neural networks in the Visual Cortex</a></li>
<li><a  class="external text" title="http://www.che.utexas.edu/~john/research/isat.htm">In Situ Adaptive Tabulation:</a> A neural network alternative.</li>
</ul>

<!-- 
Pre-expand include size: 68281 bytes
Post-expand include size: 15873 bytes
Template argument size: 20152 bytes
Maximum: 2048000 bytes
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1729542-0!1!0!default!!en!2 and timestamp 20060909195154 -->
<div class="printfooter">
Retrieved from "<a </div>
			<div id="catlinks"><p class='catlinks'><a  title="Special:Categories">Categories</a>: <span dir='ltr'><a  title="Category:Computational neuroscience">Computational neuroscience</a></span> | <span dir='ltr'><a  title="Category:Neural networks">Neural networks</a></span> | <span dir='ltr'><a  title="Category:Artificial intelligence">Artificial intelligence</a></span> | <span dir='ltr'><a  title="Category:Network architecture">Network architecture</a></span> | <span dir='ltr'><a  title="Category:Networks">Networks</a></span></p></div>			<!-- end content -->
			<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<ul>
				 <li id="ca-nstab-main" class="selected"><a href="/wiki/Neural_network.html">Article</a></li>
				 <li id="ca-talk"><a >Discussion</a></li>
				 <li id="ca-edit"><a >Edit this page</a></li>
				 <li id="ca-history"><a >History</a></li>
		</ul>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a >Sign in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(/images/wiki-en.png);" href="/wiki/Main_Page.html" title="Main Page"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
		<div class='portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage"><a href="/wiki/Main_Page.html">Main Page</a></li>
				<li id="n-portal"><a >Community Portal</a></li>
				<li id="n-Featured-articles"><a >Featured articles</a></li>
				<li id="n-currentevents"><a >Current events</a></li>
				<li id="n-recentchanges"><a >Recent changes</a></li>
				<li id="n-randompage"><a >Random article</a></li>
				<li id="n-help"><a >Help</a></li>
				<li id="n-contact"><a >Contact Wikipedia</a></li>
				<li id="n-sitesupport"><a >Donations</a></li>
			</ul>
		</div>
	</div>
		<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="/wiki/Special:Search" id="searchform"><div>
				<input id="searchInput" name="search" type="text" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" value="Search" />
			</div></form>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a >What links here</a></li>
				<li id="t-recentchangeslinked"><a >Related changes</a></li>
<li id="t-upload"><a >Upload file</a></li>
<li id="t-specialpages"><a >Special pages</a></li>
				<li id="t-print"><a >Printable version</a></li>				<li id="t-permalink"><a >Permanent link</a></li><li id="t-cite"><a >Cite this article</a></li>			</ul>
		</div>
	</div>
	<div id="p-lang" class="portlet">
		<h5>In other languages</h5>
		<div class="pBody">
			<ul>
				<li class="interwiki-ar"><a >العربية</a></li>
				<li class="interwiki-bg"><a >Български</a></li>
				<li class="interwiki-de"><a >Deutsch</a></li>
				<li class="interwiki-es"><a >Español</a></li>
				<li class="interwiki-fr"><a >Français</a></li>
				<li class="interwiki-hr"><a >Hrvatski</a></li>
				<li class="interwiki-it"><a >Italiano</a></li>
				<li class="interwiki-ko"><a >한국어</a></li>
				<li class="interwiki-ja"><a >日本語</a></li>
				<li class="interwiki-nl"><a >Nederlands</a></li>
				<li class="interwiki-ro"><a >Română</a></li>
				<li class="interwiki-pl"><a >Polski</a></li>
				<li class="interwiki-pt"><a >Português</a></li>
				<li class="interwiki-ru"><a >Русский</a></li>
				<li class="interwiki-sl"><a >Slovenščina</a></li>
				<li class="interwiki-fi"><a >Suomi</a></li>
				<li class="interwiki-sv"><a >Svenska</a></li>
				<li class="interwiki-zh"><a >中文</a></li>
			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a ><img src="/skins-1.5/common/images/poweredby_mediawiki_88x31.png" alt="MediaWiki" /></a></div>
				<div id="f-copyrightico"><a ><img src="/images/wikimedia-button.png" border="0" alt="Wikimedia Foundation"/></a></div>
			<ul id="f-list">
				<li id="lastmod"> This page was last modified 08:39, 7 September 2006.</li>
				<li id="copyright">All text is available under the terms of the <a class='internal'  title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class='internal'  title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br /> Wikipedia&reg; is a registered trademark of the Wikimedia Foundation, Inc.<br /></li>
				<li id="privacy"><a  title="wikimedia:Privacy policy">Privacy policy</a></li>
				<li id="about"><a  title="Wikipedia:About">About Wikipedia</a></li>
				<li id="disclaimer"><a  title="Wikipedia:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
		
	
		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
</div>
<!-- Served by srv55 in 0.120 secs. --></body></html>
